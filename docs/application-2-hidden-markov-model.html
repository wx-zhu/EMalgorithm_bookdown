<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Topic 6 Application 2: Hidden Markov Model | EM Algorithm</title>
  <meta name="description" content="Topic 6 Application 2: Hidden Markov Model | EM Algorithm" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Topic 6 Application 2: Hidden Markov Model | EM Algorithm" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 6 Application 2: Hidden Markov Model | EM Algorithm" />
  
  
  

<meta name="author" content="Wenxuan Zhu, Jingyi Guan, Sarah Tannert-Lerner" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="application-1-gaussian-mixture-model.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SHORT TITLE HERE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome!</a></li>
<li class="chapter" data-level="2" data-path="application-in-image-segmentation.html"><a href="application-in-image-segmentation.html"><i class="fa fa-check"></i><b>2</b> Application in Image Segmentation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="application-in-image-segmentation.html"><a href="application-in-image-segmentation.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="application-in-image-segmentation.html"><a href="application-in-image-segmentation.html#latent-variable"><i class="fa fa-check"></i><b>2.2</b> Latent variable</a></li>
<li class="chapter" data-level="2.3" data-path="application-in-image-segmentation.html"><a href="application-in-image-segmentation.html#why-em"><i class="fa fa-check"></i><b>2.3</b> Why EM?</a></li>
<li class="chapter" data-level="2.4" data-path="application-in-image-segmentation.html"><a href="application-in-image-segmentation.html#example"><i class="fa fa-check"></i><b>2.4</b> Example</a></li>
<li class="chapter" data-level="2.5" data-path="application-in-image-segmentation.html"><a href="application-in-image-segmentation.html#related-work"><i class="fa fa-check"></i><b>2.5</b> Related Work</a></li>
<li class="chapter" data-level="2.6" data-path="application-in-image-segmentation.html"><a href="application-in-image-segmentation.html#references"><i class="fa fa-check"></i><b>2.6</b> References:</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-em.html"><a href="introduction-to-em.html"><i class="fa fa-check"></i><b>3</b> Introduction to EM</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-em.html"><a href="introduction-to-em.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-em.html"><a href="introduction-to-em.html#em-algorithm-given-condition-limitation"><i class="fa fa-check"></i><b>3.2</b> EM Algorithm Given Condition &amp; Limitation</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-em.html"><a href="introduction-to-em.html#em-steps-5-step-version"><i class="fa fa-check"></i><b>3.3</b> EM Steps (5-step version)</a></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-em.html"><a href="introduction-to-em.html#em-steps-2-key-step-version"><i class="fa fa-check"></i><b>3.4</b> EM Steps (2-key-step version)</a></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-em.html"><a href="introduction-to-em.html#references-1"><i class="fa fa-check"></i><b>3.5</b> References:</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="example-binomial-mixture-model.html"><a href="example-binomial-mixture-model.html"><i class="fa fa-check"></i><b>4</b> Example: Binomial Mixture Model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="example-binomial-mixture-model.html"><a href="example-binomial-mixture-model.html#overview-of-binomial-mixture-model"><i class="fa fa-check"></i><b>4.1</b> Overview of Binomial Mixture Model</a></li>
<li class="chapter" data-level="4.2" data-path="example-binomial-mixture-model.html"><a href="example-binomial-mixture-model.html#flipping-coins-example-illustrated"><i class="fa fa-check"></i><b>4.2</b> Flipping Coins Example Illustrated</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="example-binomial-mixture-model.html"><a href="example-binomial-mixture-model.html#scenario"><i class="fa fa-check"></i><b>4.2.1</b> Scenario</a></li>
<li class="chapter" data-level="4.2.2" data-path="example-binomial-mixture-model.html"><a href="example-binomial-mixture-model.html#algorithm-steps"><i class="fa fa-check"></i><b>4.2.2</b> Algorithm steps</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="example-binomial-mixture-model.html"><a href="example-binomial-mixture-model.html#implementation-in-r"><i class="fa fa-check"></i><b>4.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="example-binomial-mixture-model.html"><a href="example-binomial-mixture-model.html#simplified-case-2-coin"><i class="fa fa-check"></i><b>4.3.1</b> Simplified case (2-coin)</a></li>
<li class="chapter" data-level="4.3.2" data-path="example-binomial-mixture-model.html"><a href="example-binomial-mixture-model.html#generalized-case-k-coin"><i class="fa fa-check"></i><b>4.3.2</b> Generalized case (k-coin)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="example-binomial-mixture-model.html"><a href="example-binomial-mixture-model.html#references-2"><i class="fa fa-check"></i><b>4.4</b> References:</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="application-1-gaussian-mixture-model.html"><a href="application-1-gaussian-mixture-model.html"><i class="fa fa-check"></i><b>5</b> Application 1: Gaussian Mixture Model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="application-1-gaussian-mixture-model.html"><a href="application-1-gaussian-mixture-model.html#why-using-gmms"><i class="fa fa-check"></i><b>5.1</b> Why using GMMs?</a></li>
<li class="chapter" data-level="5.2" data-path="application-1-gaussian-mixture-model.html"><a href="application-1-gaussian-mixture-model.html#train-gmm-using-mle"><i class="fa fa-check"></i><b>5.2</b> Train GMM using MLE</a></li>
<li class="chapter" data-level="5.3" data-path="application-1-gaussian-mixture-model.html"><a href="application-1-gaussian-mixture-model.html#train-simple-gmm-example-using-em"><i class="fa fa-check"></i><b>5.3</b> Train simple GMM example using EM</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="application-1-gaussian-mixture-model.html"><a href="application-1-gaussian-mixture-model.html#set-up"><i class="fa fa-check"></i><b>5.3.1</b> Set up</a></li>
<li class="chapter" data-level="5.3.2" data-path="application-1-gaussian-mixture-model.html"><a href="application-1-gaussian-mixture-model.html#em-steps-two-key-step-version"><i class="fa fa-check"></i><b>5.3.2</b> EM steps (Two-key-step version)</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="application-1-gaussian-mixture-model.html"><a href="application-1-gaussian-mixture-model.html#r-code-example"><i class="fa fa-check"></i><b>5.4</b> R code example</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="application-1-gaussian-mixture-model.html"><a href="application-1-gaussian-mixture-model.html#simple-example-with-2-components-in-1-dimension"><i class="fa fa-check"></i><b>5.4.1</b> Simple example with 2 components in 1 dimension</a></li>
<li class="chapter" data-level="5.4.2" data-path="application-1-gaussian-mixture-model.html"><a href="application-1-gaussian-mixture-model.html#general-case"><i class="fa fa-check"></i><b>5.4.2</b> General case</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="application-1-gaussian-mixture-model.html"><a href="application-1-gaussian-mixture-model.html#references-3"><i class="fa fa-check"></i><b>5.5</b> References:</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html"><i class="fa fa-check"></i><b>6</b> Application 2: Hidden Markov Model</a>
<ul>
<li class="chapter" data-level="6.1" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html#overview-of-hidden-markov-model"><i class="fa fa-check"></i><b>6.1</b> Overview of Hidden Markov Model</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html#markov-chain"><i class="fa fa-check"></i><b>6.1.1</b> Markov Chain</a></li>
<li class="chapter" data-level="6.1.2" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html#hidden-markov-model"><i class="fa fa-check"></i><b>6.1.2</b> Hidden Markov Model</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html#baum-welch-algorithm"><i class="fa fa-check"></i><b>6.2</b> Baum-Welch Algorithm</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html#intuition"><i class="fa fa-check"></i><b>6.2.1</b> Intuition</a></li>
<li class="chapter" data-level="6.2.2" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html#algorithm-in-steps"><i class="fa fa-check"></i><b>6.2.2</b> Algorithm in steps</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html#implementation-in-r-1"><i class="fa fa-check"></i><b>6.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html#hmm-package"><i class="fa fa-check"></i><b>6.3.1</b> HMM package</a></li>
<li class="chapter" data-level="6.3.2" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html#revisit-weather-grass-example"><i class="fa fa-check"></i><b>6.3.2</b> Revisit Weather-Grass Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="application-2-hidden-markov-model.html"><a href="application-2-hidden-markov-model.html#references-4"><i class="fa fa-check"></i><b>6.4</b> References:</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">EM Algorithm</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="application-2-hidden-markov-model" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Topic 6</span> Application 2: Hidden Markov Model<a href="application-2-hidden-markov-model.html#application-2-hidden-markov-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="overview-of-hidden-markov-model" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Overview of Hidden Markov Model<a href="application-2-hidden-markov-model.html#overview-of-hidden-markov-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="markov-chain" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Markov Chain<a href="application-2-hidden-markov-model.html#markov-chain" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Markov Chain Intuition</strong></p>
<p>Before formally defining what a Markov chain is, consider this:</p>
<p>Imagine you’re a weather forecaster and you want to predict whether it will be sunny, cloudy, or rainy tomorrow based on the weather conditions of the past few days. Let’s assume that the probability of tomorrow’s weather only depends on today’s weather and not on the weather before that.</p>
<p>Suppose that, if it’s sunny today, there is a 60% chance of it being sunny tomorrow, a 30% chance of it being cloudy, and a 10% chance of it being rainy.
<span class="math display">\[
\begin{align*}
&amp;\mathbb{P}(\text{sunny tmr}\mid \text{sunny today}) = 0.6 \\
&amp;\mathbb{P}(\text{cloudy tmr}\mid \text{sunny today}) = 0.3 \\
&amp;\mathbb{P}(\text{rainy tmr}\mid \text{sunny today}) = 0.1 \\
\end{align*}
\]</span>
If it’s cloudy today, there is a 20% chance of it being sunny tomorrow, a 70% chance of it still being cloudy, and a 10% chance of it being rainy.
<span class="math display">\[
\begin{align*}
&amp;\mathbb{P}(\text{sunny tmr}\mid \text{cloudy today}) = 0.2 \\
&amp;\mathbb{P}(\text{cloudy tmr}\mid \text{cloudy today}) = 0.7 \\
&amp;\mathbb{P}(\text{rainy tmr}\mid \text{cloudy today}) = 0.1 \\
\end{align*}
\]</span>
If it’s rainy today, there is a 30% chance of it being sunny tomorrow, a 40% chance of it still being cloudy, and a 30% chance of it being rainy.
<span class="math display">\[
\begin{align*}
&amp;\mathbb{P}(\text{sunny tmr}\mid \text{rainy today}) = 0.3 \\
&amp;\mathbb{P}(\text{cloudy tmr}\mid \text{rainy today}) = 0.4 \\
&amp;\mathbb{P}(\text{rainy tmr}\mid \text{rainy today}) = 0.3 \\
\end{align*}
\]</span></p>
<p><img src="image/pic5.png" width="85%" style="display: block; margin: auto;" /></p>
<p>In informal “mathematical” notation, we can represent this as:
<span class="math display">\[
\text{Weather States} = \{\text{Sunny}, \text{Cloudy}, \text{Rainy}\}
\]</span></p>
<p><span class="math display">\[
\text{Transition Probabilities Matrix} = \begin{bmatrix}
0.6 &amp; 0.3 &amp; 0.1 \\
0.2 &amp; 0.7 &amp; 0.1 \\
0.3 &amp; 0.4 &amp; 0.3
\end{bmatrix}
\]</span></p>
<p>In this case (and with our assumptions above), the sequence of the weather states overtime, say {Sunny, Sunny, Sunny, Cloudy, Cloudy, Rainy, Cloudy,…}, is a classic Markov chain. This is because the weather tomorrow only depends on the weather today, or more broadly, the state at time <span class="math inline">\(t+1\)</span> only depends on the state at time <span class="math inline">\(t\)</span>.</p>
<p><strong>Markov Chain Definition</strong></p>
<p>Mathematically, we can denote a Markov chain by</p>
<p><span class="math display">\[
X=(X_n)_{n\in\mathbb{N}} = (X_0, X_1, X_2,...)
\]</span></p>
<p>where at each timestamp the process takes its values in a discrete set <span class="math inline">\(E\)</span> such that</p>
<p><span class="math display">\[
X_n \in E \quad \forall n\in \mathbb{N}
\]</span></p>
<p>Then, the Markov property implies that we have
<span class="math display">\[
\mathbb{P}(X_{n+1}=s_{n+1}\mid X_n = s_n, X_{n-1}=s_{n-1},X_{n-2}=s_{n-2},\cdots) = \mathbb{P}(X_{n+1}=s_{n+1}\mid X_n = s_n)
\]</span></p>
</div>
<div id="hidden-markov-model" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Hidden Markov Model<a href="application-2-hidden-markov-model.html#hidden-markov-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Hidden Markov Model Intuition</strong></p>
<p>To illustrate Hidden Markov Model, we can continue with the weather example. Let’s assume that we are observing whether the grass is wet or dry every day, but we don’t directly observe the weather. That being said, we only observe the effect of the weather on the grass, and we assume whether the grass is wet or dry only depends on the weather that day.</p>
<p><img src="image/pic6.png" width="85%" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[
\text{Weather States} = \{\text{Sunny}, \text{Cloudy}, \text{Rainy}\}
\]</span></p>
<p><span class="math display">\[
\text{Grass States} = \{\text{Dry}, \text{Wet}\}
\]</span></p>
<p>Suppose that, if it’s sunny today, there is a <span class="math inline">\(100 \%\)</span> chance of observing dry grass, a <span class="math inline">\(0 \%\)</span> chance of observing wet grass.</p>
<p><span class="math display">\[
\mathbb{P}(\text{dry grass}\mid \text{Sunny}) = 1 \\
\mathbb{P}(\text{wet grass}\mid \text{Sunny}) = 0
\]</span></p>
<p>Suppose that, if it’s sunny today, there is a <span class="math inline">\(70 \%\)</span> chance of observing dry grass, a <span class="math inline">\(30 \%\)</span> chance of observing wet grass.</p>
<p><span class="math display">\[
\mathbb{P}(\text{dry grass}\mid \text{Cloudy}) = 0.7 \\
\mathbb{P}(\text{wet grass}\mid \text{Cloudy}) = 0.3
\]</span></p>
<p>Suppose that, if it’s rainy today, there is a <span class="math inline">\(10 \%\)</span> chance of observing dry grass, a <span class="math inline">\(90 \%\)</span> chance of observing wet grass.</p>
<p><span class="math display">\[
\mathbb{P}(\text{dry grass}\mid \text{Cloudy}) = 0.1 \\
\mathbb{P}(\text{wet grass}\mid \text{Cloudy}) = 0.9
\]</span></p>
<p>Aggregating the probabilities above into one matrix to get the emission probabilities matrix.
<span class="math display">\[
\text{Emission Probabilities Matrix} =
\begin{bmatrix}
1 &amp; 0 \\
0.7 &amp; 0.3 \\
0.1 &amp; 0.9
\end{bmatrix}
\]</span></p>
<p><strong>Hidden Markov Model Definition</strong></p>
<p>A Hidden Markov Model (HMM) is a statistical model that represents a sequence of observable events,
<span class="math display">\[
Y = (Y_t)_{1\leq t\leq T} = (Y_1, Y_2, ..., Y_T)
\]</span></p>
<p>,where each observation <span class="math inline">\(Y_t\)</span> is generated by an unknown or hidden state sequence that is a Markov chain
<span class="math display">\[
X = (X_t)_{1\leq t\leq T} = (X_1, X_2, ..., X_T)
\]</span>
, which is not directly observed.</p>
<p>Formally, an HMM is defined by the following parameters:</p>
<ol style="list-style-type: decimal">
<li>Hidden states in sequence X take values in a finite set <span class="math inline">\((h_1,h_2,...,h_N)\)</span>.</li>
<li>Observations in sequence Y take values in a finite set <span class="math inline">\((o_1,o_2,...,o_M)\)</span>.</li>
<li>An initial probability distribution over the hidden states <span class="math inline">\(X = (X_t)_{1\leq t\leq T}\)</span>, which is given by <span class="math inline">\(\pi = (\pi_1, \pi_2, ..., \pi_N)\)</span> such that <span class="math inline">\(\pi_i = \mathbb{P}(X_1 = h_i)\)</span>.</li>
<li>A transition probability matrix <span class="math inline">\(A_{N\times N}\)</span> = {<span class="math inline">\(a_{ij}\)</span>}, where entry <span class="math inline">\(a_{ij}\)</span> represents the probability of transitioning from state <span class="math inline">\(h_i\)</span> to state <span class="math inline">\(h_j\)</span>.
<span class="math display">\[
a_{ij} = \mathbb{P}(X_{t+1} = h_j \mid X_{t} = h_i)
\]</span></li>
<li>An emission probability matrix <span class="math inline">\(B_{N\times M}\)</span> = {<span class="math inline">\(b_{j(k)}\)</span>}, where <span class="math inline">\(b_{jk} = b_{j}(o_k)\)</span> represents the probability of observing the <span class="math inline">\(o_k\)</span> given that the system is in state <span class="math inline">\(h_j\)</span>.
<span class="math display">\[
b_{jk} = b_{j}(o_{k}) = \mathbb{P}(Y_t = o_k \mid X_t = h_j)
\]</span>
The goal of an HMM is to estimate the hidden state sequence X given the observed sequence Y, or to compute the likelihood of the observed sequence Y given the model parameters <span class="math inline">\((\pi, A, B)\)</span>. In fact, we can describe a hidden markov model by <span class="math inline">\(\theta = (\pi, A, B)\)</span>. In the next section, we will apply EM algorithm to estimate parameters of HMM model, which has a specific name – Baum-Welch Algorithm.</li>
</ol>
</div>
</div>
<div id="baum-welch-algorithm" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Baum-Welch Algorithm<a href="application-2-hidden-markov-model.html#baum-welch-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="intuition" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Intuition<a href="application-2-hidden-markov-model.html#intuition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Baum-Welch algorithm finds a local maximum for <span class="math inline">\(\theta^{*} = \text{argmax}_{\theta}\mathbb{P}(Y\mid \theta)\)</span>. To put it in words, it finds the set of parameters <span class="math inline">\(\theta^{*}\)</span> that makes the observed sequence <span class="math inline">\(Y\)</span> mostly likely to have occurred.</p>
</div>
<div id="algorithm-in-steps" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Algorithm in steps<a href="application-2-hidden-markov-model.html#algorithm-in-steps" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Step 1: Initial Guess</strong></p>
<p>Set <span class="math inline">\(\theta^{t=0} = (A^{t=0}, B^{t=0}, \pi^{t=0})\)</span>, where the guesses could be random or not depending on if no prior knowledge is available.</p>
<p><strong>Step 2: Forward-Backward Algorithm</strong></p>
<p>Let’s define some new variables for each observation in the sequence here – the forward probabilities and the backward probabilities.</p>
<blockquote>
<p><strong>Definition of forward and backward probabilities</strong></p>
</blockquote>
<p>For each observation <span class="math inline">\(Y_{t} \in Y\)</span>, the forward probabilities, denoted as <span class="math inline">\(\alpha_t(i)\)</span>, which represent the probability of <span class="math inline">\(Y_{t}\)</span> being in state <span class="math inline">\(o_i\)</span> at time <span class="math inline">\(t\)</span> given the observations up to time <span class="math inline">\(t\)</span>, <span class="math inline">\((Y_1, Y_2,...,Y_{t-1})\)</span>.</p>
<p><span class="math display">\[
\alpha_t(i) = \mathbb{P}(Y_t = o_i\mid Y_1 = y_1, Y_2 = y_2,...,Y_{t-1} = y_{t-1})
\]</span></p>
<p>, where <span class="math inline">\(y = (y_t)_{0&lt;t\leq T}\)</span> is the true observation sequence.</p>
<p>On the other hand, the backward probabilities, denoted as <span class="math inline">\(\beta_t(i)\)</span>, represent the probability of starting from state <span class="math inline">\(o_i\)</span> at time <span class="math inline">\(t\)</span> and generating the remaining observations from time <span class="math inline">\(t+1\)</span> to the end of the sequence, <span class="math inline">\((Y_{t+1},...,Y_T)\)</span>.</p>
<p><span class="math display">\[
\beta_t(i) = \mathbb{P}(Y_t = o_i\mid Y_{t+1} = y_{t+1}, Y_{t+2} = y_{t+2},...,Y_{T} = y_{T})
\]</span></p>
<blockquote>
<p><strong>Computation of forward and backward probabilities</strong></p>
</blockquote>
<p>The forward probabilities can be computed using the following recursive formulas:
<span class="math display">\[
\begin{aligned}
    \alpha_1(i) &amp;= \pi_i \cdot b_i(y_1), \quad 1\leq i \leq N\\
    \alpha_t(i) &amp;= b_i(y_t) \sum_{j=1}^{N} \alpha_{t-1}(j) \cdot a_{ji}, \quad 1\leq i \leq N, \quad 2 \leq t \leq T \\
\end{aligned}
\]</span>
, where <span class="math inline">\(N\)</span> is the number of states in the HMM, <span class="math inline">\(a_{ij}\)</span> is the transition probability from state <span class="math inline">\(h_i\)</span> to state <span class="math inline">\(h_j\)</span>, <span class="math inline">\(b_i(y_t)\)</span> is the emission probability of observation <span class="math inline">\(y_t\)</span> from state <span class="math inline">\(h_i\)</span>, and <span class="math inline">\(y_t\)</span> is the true observation at time <span class="math inline">\(t\)</span>.</p>
<p>The backward probabilities can be computed using similar recursive formulas:
<span class="math display">\[
\begin{aligned}
\beta_T(i) &amp;= 1, \quad 1\leq i \leq N\\
    \beta_t(i) &amp;= \sum_{j=1}^{N} a_{ij} \cdot b_j(y_{t+1}) \cdot \beta_{t+1}(j), \quad 1\leq i \leq N,\quad 1 \leq t \leq T-1
\end{aligned}
\]</span>
, where <span class="math inline">\(N\)</span> is the number of states in the HMM, <span class="math inline">\(a_{ij}\)</span> is the transition probability from state <span class="math inline">\(h_i\)</span> to state <span class="math inline">\(h_j\)</span>, <span class="math inline">\(b_i(y_t)\)</span> is the emission probability of observation <span class="math inline">\(y_t\)</span> from state <span class="math inline">\(h_i\)</span>, and <span class="math inline">\(y_t\)</span> is the true observation at time <span class="math inline">\(t\)</span>.</p>
<p><strong>Step 3: E-step</strong></p>
<p>Calculate two new sets of quantities:</p>
<p><span class="math inline">\(\gamma_t(i)\)</span> represents the probability of the hidden state at time <span class="math inline">\(t\)</span>, <span class="math inline">\(X_t\)</span>, being in state <span class="math inline">\(h_i\)</span> given the entire observation sequence <span class="math inline">\(Y=y\)</span> and parameter set <span class="math inline">\(\theta\)</span>, and it can be calculated using Bayes’ theorem.</p>
<p><span class="math display">\[
\begin{aligned}
\gamma_t(i) &amp;= \mathbb{P}(X_t = h_i\mid Y = y,\theta) \\
&amp;= \frac{\mathbb{P}(X_t = h_i, Y = y \mid \theta)}{\mathbb{P}(Y=y \mid \theta)} \\
&amp;= \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^{N}\alpha_t(j)\beta_t(j)}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\xi_t(i,j)\)</span> represents probability of the hidden state being in state <span class="math inline">\(h_i\)</span> at time <span class="math inline">\(t\)</span> and and being in state <span class="math inline">\(h_j\)</span> at time <span class="math inline">\(t+1\)</span> given the the entire observation sequence <span class="math inline">\(Y=y\)</span> and parameter set <span class="math inline">\(\theta\)</span>, and it can also be calculated using the Bayes’ Rule.</p>
<p><span class="math display">\[
\begin{aligned}
\xi_t(i,j) &amp;= \mathbb{P}(X_t=h_i, X_{t+1}=h_j\mid Y=y,\theta) \\
&amp;= \frac{\mathbb{P}(X_t = h_i, X_{t+1}=h_j,Y=y\mid \theta)}{\mathbb{P}(Y=y\mid \theta)} \\
&amp;= \frac{\alpha_t(i)a_{ij}\beta_{t+1}(j)b_j(y_{t+1})}{\sum_{k=1}^{N}\sum_{w=1}^{N}\alpha_t(k)a_{kw}\beta_{t+1}(w)b_w(y_{t+1})}
\end{aligned}
\]</span></p>
<p>Notice here, that the denominators of <span class="math inline">\(\gamma_t(i)\)</span> and <span class="math inline">\(\xi_t(i,j)\)</span> are the same, and it calculates the probability of observing the sequence <span class="math inline">\(y\)</span> that we observed under our current guess for <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Step 4: M-step</strong></p>
<p>Now, we can update the parameters of the hidden markov model.</p>
<p>First, let’s update the initial probability distribution <span class="math inline">\(\pi = (\pi_1, \pi_2, ..., \pi_N)\)</span>.
<span class="math display">\[
\begin{aligned}
\hat{\pi}_i &amp;= \gamma_1(i) \\
&amp;= \mathbb{P}(X_1 = h_i) \\
\\
\hat{\pi} &amp;= (\gamma_1(1), \gamma_1(2),...,\gamma_1(N)) \\
&amp;= (\mathbb{P}(X_1 = h_1), \mathbb{P}(X_1 = h_2),...,\mathbb{P}(X_1 = h_N)) \\
\end{aligned}
\]</span>
Next, update the <span class="math inline">\(a_{ij}\)</span> entry in the transition matrix <span class="math inline">\(A\)</span> with the expected number of transitions from state <span class="math inline">\(h_i\)</span> to state <span class="math inline">\(h_j\)</span> divided by the expected total number of transitions from state <span class="math inline">\(h_i\)</span> including transitions to itself which equals to the number of times state <span class="math inline">\(h_i\)</span> appears in the observed sequence <span class="math inline">\(y\)</span>.</p>
<p><span class="math display">\[
\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\xi_{t}(i,j)}{\sum_{t=1}^{T-1}\gamma_{t}(i)}
\]</span></p>
<p>Last but not least, update <span class="math inline">\(b_{ij}\)</span> entry in the emission matrix <span class="math inline">\(B\)</span> with the expected number of times the output observations have been equal to <span class="math inline">\(o_j\)</span> under hidden state <span class="math inline">\(h_i\)</span> over the expected total number of <span class="math inline">\(h_i\)</span> in the hidden states sequence <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[
\hat{b}_i(o_k) = \frac{\sum_{t=1}^{T}1_{y_t = o_k}\gamma_i(t)}{\sum_{t=1}^{T}\gamma_i(t)}
\]</span>
, where</p>
<p><span class="math display">\[
1_{y_t = o_k} = \Bigg\{^{ 1 \text{ if y_t = o_k}}_{0 \text{ otherwise}}
\]</span></p>
<p><strong>Step 5: Determine convergence status and repeat iteratively if needed</strong></p>
<p>Set a criteria for whether the estimation sequence is converging and our current guess is close enough to last guess. If not, then repeat from step 2 to step 5 iteratively until convergence.</p>
<p>Alternatively, we can set a maximum number of iteration for the Baum-Welch algorithm, n. Once we have completed n iterations, the algorithm stops and we take the estimates from the last iteration as results.</p>
</div>
</div>
<div id="implementation-in-r-1" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Implementation in R<a href="application-2-hidden-markov-model.html#implementation-in-r-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="hmm-package" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> HMM package<a href="application-2-hidden-markov-model.html#hmm-package" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>HMM package documentation: <a href="https://cran.r-project.org/web/packages/HMM/HMM.pdf" class="uri">https://cran.r-project.org/web/packages/HMM/HMM.pdf</a></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="application-2-hidden-markov-model.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages(&#39;HMM&#39;)</span></span>
<span id="cb37-2"><a href="application-2-hidden-markov-model.html#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(HMM)</span></code></pre></div>
<p>Example usage of ‘baumWelch’ function in the HMM package:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="application-2-hidden-markov-model.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize HMM with states values sets, transition probabilities, and emission probabilities</span></span>
<span id="cb38-2"><a href="application-2-hidden-markov-model.html#cb38-2" aria-hidden="true" tabindex="-1"></a>hmm <span class="ot">=</span> <span class="fu">initHMM</span>(<span class="fu">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;B&quot;</span>) <span class="co"># hidden states A and B</span></span>
<span id="cb38-3"><a href="application-2-hidden-markov-model.html#cb38-3" aria-hidden="true" tabindex="-1"></a>             ,<span class="fu">c</span>(<span class="st">&quot;L&quot;</span>,<span class="st">&quot;R&quot;</span>) <span class="co"># observation states L or R</span></span>
<span id="cb38-4"><a href="application-2-hidden-markov-model.html#cb38-4" aria-hidden="true" tabindex="-1"></a>             ,<span class="at">startProbs =</span> <span class="fu">c</span>(<span class="fl">0.5</span>,<span class="fl">0.5</span>)</span>
<span id="cb38-5"><a href="application-2-hidden-markov-model.html#cb38-5" aria-hidden="true" tabindex="-1"></a>             ,<span class="at">transProbs=</span><span class="fu">matrix</span>(<span class="fu">c</span>(.<span class="dv">9</span>,.<span class="dv">1</span>,.<span class="dv">1</span>,.<span class="dv">9</span>),<span class="dv">2</span>)</span>
<span id="cb38-6"><a href="application-2-hidden-markov-model.html#cb38-6" aria-hidden="true" tabindex="-1"></a>             ,<span class="at">emissionProbs=</span><span class="fu">matrix</span>(<span class="fu">c</span>(.<span class="dv">5</span>,.<span class="dv">51</span>,.<span class="dv">5</span>,.<span class="dv">49</span>),<span class="dv">2</span>))</span>
<span id="cb38-7"><a href="application-2-hidden-markov-model.html#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="application-2-hidden-markov-model.html#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co"># print the HMM we just built to check</span></span>
<span id="cb38-9"><a href="application-2-hidden-markov-model.html#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(hmm)</span></code></pre></div>
<pre><code>## $States
## [1] &quot;A&quot; &quot;B&quot;
## 
## $Symbols
## [1] &quot;L&quot; &quot;R&quot;
## 
## $startProbs
##   A   B 
## 0.5 0.5 
## 
## $transProbs
##     to
## from   A   B
##    A 0.9 0.1
##    B 0.1 0.9
## 
## $emissionProbs
##       symbols
## states    L    R
##      A 0.50 0.50
##      B 0.51 0.49</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="application-2-hidden-markov-model.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sequence of observation</span></span>
<span id="cb40-2"><a href="application-2-hidden-markov-model.html#cb40-2" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">&quot;L&quot;</span>,<span class="dv">100</span>),<span class="fu">rep</span>(<span class="st">&quot;R&quot;</span>,<span class="dv">300</span>)))</span>
<span id="cb40-3"><a href="application-2-hidden-markov-model.html#cb40-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">&quot;L&quot;</span>,<span class="dv">300</span>),<span class="fu">rep</span>(<span class="st">&quot;R&quot;</span>,<span class="dv">100</span>)))</span>
<span id="cb40-4"><a href="application-2-hidden-markov-model.html#cb40-4" aria-hidden="true" tabindex="-1"></a>observation <span class="ot">=</span> <span class="fu">c</span>(a,b) <span class="co"># append vector b after vector a</span></span></code></pre></div>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="application-2-hidden-markov-model.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Baum-Welch</span></span>
<span id="cb41-2"><a href="application-2-hidden-markov-model.html#cb41-2" aria-hidden="true" tabindex="-1"></a>bw <span class="ot">=</span> <span class="fu">baumWelch</span>(hmm,observation, <span class="at">maxIteration =</span> <span class="dv">10</span>) <span class="co"># Input: hmm, observation, maxIteration (the maximum number of iterations in the Baum-Welch algorithm)</span></span>
<span id="cb41-3"><a href="application-2-hidden-markov-model.html#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(bw<span class="sc">$</span>hmm)</span></code></pre></div>
<pre><code>## $States
## [1] &quot;A&quot; &quot;B&quot;
## 
## $Symbols
## [1] &quot;L&quot; &quot;R&quot;
## 
## $startProbs
##   A   B 
## 0.5 0.5 
## 
## $transProbs
##     to
## from            A          B
##    A 9.974699e-01 0.00253015
##    B 6.670293e-06 0.99999333
## 
## $emissionProbs
##       symbols
## states         L         R
##      A 0.2464713 0.7535287
##      B 0.7486563 0.2513437</code></pre>
<p>Notice here that one drawback of the baumWelch function in the HMM package is that it treats the initial probability vector we put in as true initial probability rather than our random guess. Thus, it does not try to update initial probability, which lower the accuracy of the estimation when the initial probability vector we put in is not that accurate.</p>
</div>
<div id="revisit-weather-grass-example" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Revisit Weather-Grass Example<a href="application-2-hidden-markov-model.html#revisit-weather-grass-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s do a simulation to test the performance of Baum-Welch algorithm here.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="application-2-hidden-markov-model.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize HMM with states values sets, transition probabilities, and emission probabilities</span></span>
<span id="cb43-2"><a href="application-2-hidden-markov-model.html#cb43-2" aria-hidden="true" tabindex="-1"></a>hmm <span class="ot">=</span> <span class="fu">initHMM</span>(<span class="fu">c</span>(<span class="st">&quot;Sunny&quot;</span>,<span class="st">&quot;Cloudy&quot;</span>,<span class="st">&quot;Rainy&quot;</span>), <span class="co"># hidden states: WEATHER * 3</span></span>
<span id="cb43-3"><a href="application-2-hidden-markov-model.html#cb43-3" aria-hidden="true" tabindex="-1"></a>              <span class="fu">c</span>(<span class="st">&quot;Dry&quot;</span>,<span class="st">&quot;Wet&quot;</span>), <span class="co"># observation states: GRASS STATUS * 2</span></span>
<span id="cb43-4"><a href="application-2-hidden-markov-model.html#cb43-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">startProbs =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>,<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>,<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>),</span>
<span id="cb43-5"><a href="application-2-hidden-markov-model.html#cb43-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">transProbs =</span> <span class="fu">cbind</span>(<span class="fu">c</span>(.<span class="dv">6</span>,.<span class="dv">2</span>,.<span class="dv">3</span>),<span class="fu">c</span>(.<span class="dv">3</span>,.<span class="dv">4</span>,.<span class="dv">7</span>),<span class="fu">c</span>(.<span class="dv">1</span>,.<span class="dv">1</span>,.<span class="dv">3</span>)),</span>
<span id="cb43-6"><a href="application-2-hidden-markov-model.html#cb43-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">emissionProbs=</span><span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>),<span class="fu">c</span>(.<span class="dv">7</span>,.<span class="dv">3</span>),<span class="fu">c</span>(.<span class="dv">1</span>,.<span class="dv">9</span>)))</span>
<span id="cb43-7"><a href="application-2-hidden-markov-model.html#cb43-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-8"><a href="application-2-hidden-markov-model.html#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="co"># print the HMM we just built to check</span></span>
<span id="cb43-9"><a href="application-2-hidden-markov-model.html#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(hmm)</span></code></pre></div>
<pre><code>## $States
## [1] &quot;Sunny&quot;  &quot;Cloudy&quot; &quot;Rainy&quot; 
## 
## $Symbols
## [1] &quot;Dry&quot; &quot;Wet&quot;
## 
## $startProbs
##     Sunny    Cloudy     Rainy 
## 0.3333333 0.3333333 0.3333333 
## 
## $transProbs
##         to
## from     Sunny Cloudy Rainy
##   Sunny    0.6    0.3   0.1
##   Cloudy   0.2    0.4   0.1
##   Rainy    0.3    0.7   0.3
## 
## $emissionProbs
##         symbols
## states   Dry Wet
##   Sunny  1.0 0.0
##   Cloudy 0.7 0.3
##   Rainy  0.1 0.9</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="application-2-hidden-markov-model.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate 5000 observations and corresponding hidden states consecutively</span></span>
<span id="cb45-2"><a href="application-2-hidden-markov-model.html#cb45-2" aria-hidden="true" tabindex="-1"></a>sim <span class="ot">=</span> <span class="fu">simHMM</span>(hmm, <span class="dv">5000</span>)</span></code></pre></div>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="application-2-hidden-markov-model.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Baum-Welch</span></span>
<span id="cb46-2"><a href="application-2-hidden-markov-model.html#cb46-2" aria-hidden="true" tabindex="-1"></a>bw <span class="ot">=</span> <span class="fu">baumWelch</span>(hmm,sim<span class="sc">$</span>observation, <span class="at">maxIteration =</span> <span class="dv">10</span>)</span>
<span id="cb46-3"><a href="application-2-hidden-markov-model.html#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(bw<span class="sc">$</span>hmm)</span></code></pre></div>
<pre><code>## $States
## [1] &quot;Sunny&quot;  &quot;Cloudy&quot; &quot;Rainy&quot; 
## 
## $Symbols
## [1] &quot;Dry&quot; &quot;Wet&quot;
## 
## $startProbs
##     Sunny    Cloudy     Rainy 
## 0.3333333 0.3333333 0.3333333 
## 
## $transProbs
##         to
## from         Sunny    Cloudy     Rainy
##   Sunny  0.6481263 0.2120997 0.1397740
##   Cloudy 0.3535154 0.4445554 0.2019293
##   Rainy  0.2973958 0.4071881 0.2954161
## 
## $emissionProbs
##         symbols
## states         Dry       Wet
##   Sunny  1.0000000 0.0000000
##   Cloudy 0.7087220 0.2912780
##   Rainy  0.1058077 0.8941923</code></pre>
</div>
</div>
<div id="references-4" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> References:<a href="application-2-hidden-markov-model.html#references-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p><a href="https://en.wikipedia.org/wiki/Baum%e2%80%93Welch_algorithm">Baum welch Wikipedia</a></p></li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0167947315001759">Gaussian quadrature approximations in mixed hidden Markov models for longitudinal data: A simulation study</a></p></li>
<li><p><a href="https://cran.r-project.org/web/packages/HMM/HMM.pdf">HMM package documentation</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model">HMM wikipedia</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chain</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Markov_model">Markov model</a></p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="application-1-gaussian-mixture-model.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/wx-zhu/Stat455_EM_finalproject/edit/main/06-HMM.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/wx-zhu/Stat455_EM_finalproject/blob/main/06-HMM.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
