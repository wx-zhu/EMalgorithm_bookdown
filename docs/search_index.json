[["index.html", "EM Algorithm Topic 1 Welcome!", " EM Algorithm Wenxuan Zhu, Jingyi Guan, Sarah Tannert-Lerner Spring 2023, MATH/STAT 455, Macalester College Topic 1 Welcome! "],["introduction-to-em.html", "Topic 2 Introduction to EM 2.1 Introduction 2.2 EM Algorithm Given Condition &amp; Limitation 2.3 EM Steps (5-step version) 2.4 EM Steps (2-key-step version) 2.5 References:", " Topic 2 Introduction to EM 2.1 Introduction Given condition: We have observed variables \\(X\\), but we believe (assume) that there are some “latent” variables \\(Z\\) that we cannot observe. Usually, when given we only have knowledge about observed variables \\(X\\), we may want to calculate the MLE for the parameter of interest \\(\\theta\\) in this case: \\[ \\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\log {L(\\theta)} \\] However, for some problems, it is hard to solve the above equation. → We use EM Algorithm instead! 2.2 EM Algorithm Given Condition &amp; Limitation Given condition: We have observed variables \\(X\\), but we believe (assume) that there are some “latent” variables *\\(Z\\) that we cannot observe. Assume some parameterized distribution for the latent variables \\(Z\\) or \\(p(Z\\mid X, \\theta)\\) Limitation: EM estimate is only guaranteed to never get worse, but won’t necessarily find the global optimum of the likelihood In practice, start EM from multiple initial guesses and choose the one with the largest likelihood as the final guess for \\(\\theta\\) 2.3 EM Steps (5-step version) EM algorithm is usually described as two steps (the E-step and the M-step), but we think it’s helpful to think of EM as five distinct steps: Pick an initial guess for \\(\\theta^{t = 0}\\) for the parameter of interest \\(\\theta\\) Given the observed variable \\(X\\), calculate the conditional distribution of the observed variable \\(X\\) and latent variable \\(Z\\): \\(p(X, Z \\mid \\theta^{t})\\) Find the Q-function, which is the expected \\(\\log {p(X,Z\\mid \\theta)}\\) by using the conditional distribution of latent variable \\(Z\\) from the last step \\[ \\begin{align} Q(\\theta \\mid \\theta^t) &amp; = expected\\; \\log {p(X,Z\\mid \\theta)} \\\\ &amp; = E_{Z\\mid x,\\theta^t} [\\log{p(X,Z\\mid \\theta)}] \\\\ &amp; = \\int^{all\\; Z} \\log{p(X,Z \\mid \\theta)p(Z\\mid X,\\theta^t)} \\,dZ \\end{align} \\] Find the new guess \\(\\theta^{t+1}\\) by maximizing Q-function above Let \\(t = t+1\\) and go back to Step 2 2.4 EM Steps (2-key-step version) We also include the E-step and M-step version of EM algorithm below for reference: Start with random guess for \\(\\theta\\) E step (finding the Q-function): \\(Q(\\theta \\mid \\theta^t) = E_{Z|X,\\theta^m} [\\log{p(X, Z \\mid \\theta)}]\\) M step (finding the new guess for \\(\\theta\\) by maximizing the Q-function): \\(\\theta^{t+1} = \\underset{\\theta}{\\operatorname{argmax}} \\: Q(\\theta \\mid \\theta^t)\\) Repeat until \\(|\\theta^{t+1} - \\theta^m| &lt; \\epsilon\\) 2.5 References: EM Demystified: An Expectation-Maximization Tutorial Youtube: Expectation Maximization - 1 - Theory "],["example-binomial-mixture-model.html", "Topic 3 Example: Binomial Mixture Model 3.1 References:", " Topic 3 Example: Binomial Mixture Model 3.1 References: Expectation Maximization (EM) Algorithm - colorado.edu Youtube: Expectation Maximization - 2 - Example: Binomial Mixture Model "],["application-1-gaussian-mixture-model.html", "Topic 4 Application 1: Gaussian Mixture Model 4.1 Why using GMMs? 4.2 Train GMM using MLE 4.3 Train simple GMM example using EM 4.4 R code example 4.5 References:", " Topic 4 Application 1: Gaussian Mixture Model 4.1 Why using GMMs? Suppose we have a dataset with 2 features. When you plot your dataset, it might look like this: Figure 4.1: A dataset with 2 clusters, made by Maël Fabien From the above plot, there are 2 clusters. Thus, we are aiming to solve two tasks: Clustering of the data Modeling of the distribution for the data and the corresponding parameters. Usually, when doing clustering, people will think of k-means. But, in fact, K-means is just a special case for Gaussian Mixture Models (GMMs) when using a specific EM algorithm. The following plot shows some differences between K-means and GMMs for clustering. In general, for k-means, the clusters are defined by the data means whereas GMM, clusters are defined by data means and variance modeled as Gaussian (aka Normal distribution). Figure 4.2: K-Means vs. GMMs for clustering, made by Maël Fabien Thus, when using 2 Gaussians, we want to find: the mean and covariance of the first Gaussian the mean and covariance of the second Gaussian the weight of each Gaussian component 4.2 Train GMM using MLE When training a GMM, we want to identify a set of parameters that characterize our GMM. \\[ \\theta = (w_k, \\mu_k, \\Sigma_k), k = 1,2,3,\\dots, M \\] Similar to solve parameters in single Gaussian model using MLE, we can take the Score function \\(\\frac {dL}{d\\theta}ln L(\\theta)\\) using log-likelihood and set it equal to 0 again, where \\(\\prod^N_{i=1}\\) is all observations, \\(\\sum^M_{k=1}\\) is all components (number of clusters), \\(w_k\\) is component weights, and \\(N(x_i,\\mu_k, \\sigma^2_k)\\) are Gussians: \\[ \\begin{aligned} L(\\theta \\mid X_1,\\dots, X_n) &amp; = \\prod ^N_{i=1} \\sum^M_{k=1}w_k\\;N(x_i,\\mu_k, \\sigma^2_k) \\\\ ln L(\\theta) = \\log L(\\theta \\mid X_1,\\dots, X_n ) &amp;= \\sum^N_{i=1} \\log (\\sum^M_{k=1} w_k \\;N(x_i,\\mu_k, \\sigma^2_k)) \\\\ \\frac {dL}{d\\theta}ln L(\\theta) &amp;\\stackrel{Set}{= }0 \\end{aligned} \\] Unfortunately, the typical MLE approach reaches its limits since the above expression of Score function is analytically unsolvable. Thus, EM algorithm provides ways to overcome this unsolvable expression by maximizing the likelihood for the parameter of the interest and iterating till we have the desired estimate within the error range. 4.3 Train simple GMM example using EM Goal: Find which Gaussian component each observed data belongs to (latent variable \\(Z\\)), which can help we to identify each Gaussian parameters. 4.3.1 Set up Suppose we want to have a data set of 2 clusters in 1 dimension to simplified the example so \\(K=2\\) in \\(1D\\). Assumptions: We assume that we have observed variable \\(X\\) and some latent variable \\(Z\\) which decides which cluster the data points belong to. We also assume that the data given the cluster \\(J\\) has a Normal distributes with mean \\(\\mu_j\\) and variance \\(\\sigma^2_j\\) corresponding to that cluster. \\[ X \\mid Z = J \\sim N(\\mu_j, \\sigma^2_j) \\] Unknown parameters: In this case, we don’t know means \\(\\mu\\) and variances \\(\\sigma\\) for each clusters, and the probability of being in one of the 2 clusters \\(p\\). Since we only have 2 clusters, the probability of being in the other cluster is the complement probability of \\(p\\), or \\((1-p)\\). \\[ \\theta = (p,\\mu_0,\\mu_1,\\sigma_0,\\sigma_1) \\] 4.3.2 EM steps (Two-key-step version) Start with random guess for \\(\\theta\\) E step (finding the Q-function): \\(Q(\\theta \\mid \\theta^t) = E_{Z|X,\\theta^m} [\\log{p(x, z \\mid \\theta)}]\\) For a single observation: \\[ \\begin{aligned} p(X, Z\\mid \\theta) &amp; = p(x\\mid z, \\theta)\\;p(z\\mid\\theta) \\\\ &amp;= \\left( \\frac1{\\sqrt{2 \\pi}\\sigma_0}e^{-\\frac{(x-\\mu_0)^2}{2\\sigma_0^2}}(1-p)\\right)^{1-Z} \\;\\left( \\frac1{\\sqrt{2 \\pi}\\sigma_1}e^{-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}}p\\right)^{Z} \\end{aligned} \\] The above expression means that \\(p(x, z\\mid \\theta)\\) is equal to probability of the first cluster \\(z=0\\) times the probability of that \\((1-p)\\) and probability of the second cluster \\(z=1\\) times the probability of that \\(p\\). For a single observation: Q function \\[ \\begin{align} Q(\\theta \\mid \\theta^t) &amp; = E_{Z\\mid X,\\theta^t} [\\log{p(x,z\\mid \\theta)}] \\\\ &amp; = (1-E(z)) \\left(-\\log{\\sigma_0} - \\frac{(x-\\mu_0)^2}{x\\sigma_0^2}+\\log{(1-p)}+ const. \\right) \\\\ &amp; \\quad \\;+ E(z) \\left(-\\log{\\sigma_1} - \\frac{(x-\\mu_1)^2}{x\\sigma_1^2}+\\log{p}+ const. \\right) \\end{align} \\] Find the Q-function by calculating the expected \\(\\log {p(x, z\\mid \\theta)}\\) for a single observation. For n (iid) observation: Q function \\[ \\begin{aligned} Q(\\theta \\mid \\theta^t) &amp; = \\sum _i(1-E(z_i)) \\left(-\\log{\\sigma_0} - \\frac{(x-\\mu_0)^2}{x\\sigma_0^2}+\\log{(1-p)}+ const. \\right) \\\\ &amp; \\quad \\; + E(z_i) \\left(-\\log{\\sigma_1} - \\frac{(x-\\mu_1)^2}{x\\sigma_1^2}+\\log{p}+ const. \\right) \\end{aligned} \\] Generalize the Q-function from the above single observation by taking the sum of \\(z_i\\). For n (iid) observation: \\(E(z_i)\\) \\[ \\begin{aligned} E(z_i) &amp; = 0 \\;\\cdot \\; P(z_i = 0 \\mid x_i) + 1 \\;\\cdot \\; P(z_i = 1 \\mid x_i) = P(z_i = 1 \\mid x_i) \\\\ &amp; = \\frac{P(x_i\\mid z_i=1) P(z_i=1)}{P(x_i\\mid z_i=1) P(z_i=1) + P(x_i\\mid z_i=0) P(z_i=0)} \\\\ &amp; = \\frac{P(x_i\\mid z_i=1)\\;\\cdot\\; p}{P(x_i\\mid z_i=1)\\;\\cdot\\; p+ P(x_i\\mid z_i=0)\\;\\cdot\\; (1-p)} \\end{aligned} \\] Find the expected \\(z_i\\) in the above Q-function giving the known information of each observed \\(x_i\\). M step (finding the new guess for \\(\\theta\\) by maximizing the Q-function): \\(\\theta^{t+1} = \\underset{\\theta}{\\operatorname{argmax}} \\: Q(\\theta \\mid \\theta^t)\\) For the calculation below, we use weighed average, where the weights are the probability of being in the 1st “class”. \\[ \\begin{aligned} \\frac {dQ}{d\\mu_1} = \\sum_i E(z_i)\\left( \\frac{x_i-\\mu_1}{\\sigma_1^2} \\right) &amp; \\stackrel{Set}{= } 0 \\\\ \\therefore \\mu_1 &amp; = \\frac{\\sum E(z_i)x_i}{\\sum E(z_i)} \\\\ \\text{Similarly } \\mu_0 &amp; = \\frac{\\sum E(z_i)x_i}{\\sum E(z_i)}\\\\ \\\\ \\frac {dQ}{d\\sigma_1} = \\sum_i E(z_i)\\left( -\\frac1{\\sigma_1} +\\frac{(x_i-\\mu_1)^2}{\\sigma_1^3} \\right) &amp; \\stackrel{Set}{= } 0 \\\\ \\therefore \\sigma_1^2 &amp; = \\frac{\\sum E(z_i)(x_i-\\mu_1)^2}{\\sum E(z_i)} \\\\ \\text{Similarly } \\sigma_0^2 &amp; = \\frac{\\sum E(z_i)(x_i-\\mu_0)^2}{\\sum E(z_i)}\\\\ \\\\ \\frac {dQ}{dp} = \\sum_i \\left(-\\frac{1-E(z_i)}{1-p} + \\frac{E(z_i)}{p} \\right) &amp; \\stackrel{Set}{= } 0 \\\\ \\therefore p &amp; = \\frac{\\sum E(z_i)}{n} \\\\ \\end{aligned} \\] Repeat until \\(|\\theta^{t+1} - \\theta^m| &lt; \\epsilon\\) 4.4 R code example # EM: GMM library(ks) library(mvtnorm) library(extraDistr) library(ggplot2) library(pracma) 4.4.1 Simple example with 2 components in 1 dimension ## 1 dimension, 2 components ## sigma&#39;s, p are known; only estimate mu&#39;s set.seed(247) p = 0.3 n = 2000 # latent variable z z = rbinom(n, 1, p) # observed variables mu1 = 2 sig1= sqrt(2) mu0 = -2 sig0 = sqrt(1) x = rnorm(n, mu1, sig1)*z + rnorm(n, mu0, sig0)*(1-z) # plot data histogram hist(x, breaks = 50, col = &quot;#F3B941&quot;) # E step doE &lt;- function(theta){ mu0 = theta[1] mu1 = theta[2] E = rep(NA, n) for (i in 1:n){ a = dnorm(x[i], mean = mu1, sd = sig1)*p b = dnorm(x[i], mean = mu0, sd = sig0)*(1-p) E[i] = a/(a+b) } return(E) } # M step doM &lt;- function(E){ theta = rep(NA,2) theta[1] = sum((1-E)*x) / sum(1-E) theta[2] = sum(E*x) / sum(E) return(theta) } EM &lt;- function(theta, maxIter = 50, tol = 1e-5){ theta.t = theta for (i in 1:maxIter){ E = doE(theta.t) theta.t = doM(E) cat(&quot;Interation: &quot;, i, &quot; pt: &quot;, theta.t, &quot;\\n&quot;) if (norm(theta.t - theta, type = &quot;2&quot;) &lt; tol) break theta = theta.t } return(theta.t) } # Initial values theta0 = rmvnorm(1, mean = c(0,0)) (theta.final = EM(theta0)) ## Interation: 1 pt: 1.300618 -2.042688 ## Interation: 2 pt: 1.542635 -2.101853 ## Interation: 3 pt: 1.701525 -2.066578 ## Interation: 4 pt: 1.809297 -2.038246 ## Interation: 5 pt: 1.878507 -2.019401 ## Interation: 6 pt: 1.921065 -2.007542 ## Interation: 7 pt: 1.946516 -2.000335 ## Interation: 8 pt: 1.961482 -1.996054 ## Interation: 9 pt: 1.970195 -1.993545 ## Interation: 10 pt: 1.975238 -1.992088 ## Interation: 11 pt: 1.978148 -1.991245 ## Interation: 12 pt: 1.979822 -1.990759 ## Interation: 13 pt: 1.980786 -1.99048 ## Interation: 14 pt: 1.981339 -1.990319 ## Interation: 15 pt: 1.981657 -1.990227 ## Interation: 16 pt: 1.98184 -1.990174 ## Interation: 17 pt: 1.981945 -1.990143 ## Interation: 18 pt: 1.982005 -1.990126 ## Interation: 19 pt: 1.98204 -1.990115 ## Interation: 20 pt: 1.98206 -1.99011 ## Interation: 21 pt: 1.982071 -1.990106 ## Interation: 22 pt: 1.982078 -1.990104 ## [1] 1.982078 -1.990104 ## estimate p, mu&#39;s and sigma&#39;s 4.4.2 General case We also include the code for a more general version of EM algorithm in GMM questions for simulation and hand-on further exploration. If you are curious about the calculation for the following general case, look at the second half of the youtube video. ## K * 2D Gaussians set.seed(247) # Params K = 3 phis = rdirichlet(1, rep(1, K)) j = 2 # how far apart the centers are mus = matrix(c(j,-j,0,j,j,-j), ncol = 2) # rmvnorm(K, mean = c(0, 0)) Sigmas = vector(&quot;list&quot;, K) for (k in 1:K){ mat = matrix(rnorm(100), ncol = 2) Sigmas[[k]] = cov(mat) } # Latent z = rcat(n, phis) # Observed (generated) x = matrix(nrow = n, ncol = 2) for (i in 1:n){ k = z[i] mu = mus[k,] sigma = Sigmas[[k]] x[i,] = rmvnorm(1, mean = mu, sigma = sigma) } # plot the data df = data.frame(x = x, mu = as.factor(z)) ggplot(df, aes(x = x[,1], y = x[,2], col = mu, fill = mu)) + geom_point() + scale_color_manual(values = c(&quot;#3B5BA5&quot;, &quot;#E87A5D&quot;, &quot;#F3B941&quot;)) + theme_minimal() # E step doE &lt;- function(theta){ E = with(theta, do.call(cbind, lapply(1:K, function(k) phis[[k]]*dmvnorm(x, mus[[k]], Sigmas[[k]])))) E/rowSums(E) } # M step doM &lt;- function(E){ phis = colMeans(E) covs = lapply(1:K, function(k) cov.wt(x, E[, k], method = &quot;ML&quot;)) mus = lapply(covs, &quot;[[&quot;, &quot;center&quot;) sig = lapply(covs, &quot;[[&quot;, &quot;cov&quot;) return(list(mus = mus, Sigmas = sig, phis = phis)) } logLikelihood &lt;- function(theta){ probs = with(theta, do.call(cbind, lapply(1:K, function(i) phis[i] * dmvnorm(x, mus[[i]], Sigmas[[i]])))) sum(log(rowSums(probs))) } EM &lt;- function(theta, maxIter = 30, tol = 1e-1){ theta.t = theta for (i in 1:maxIter){ E = doE(theta.t) theta.t = doM(E) ll.diff = logLikelihood(theta.t) - logLikelihood(theta) cat(&quot;Iteration: &quot;, i, &quot; ll differnece: &quot;, ll.diff, &quot;\\n&quot;) if (abs(ll.diff) &lt; tol) break theta = theta.t } return(theta.t) } # inital values set.seed(1) phis0 = rdirichlet(1, rep(1,K)) mus0 = vector(&quot;list&quot;, K) Sigmas0 = vector(&quot;list&quot;, K) for (k in 1:K){ mat = matrix(rnorm(100), ncol = 2) mus0[[k]] = rmvnorm(1, mean = c(3,-3)) Sigmas0[[k]] = cov(mat) } theta0 = list(mus = mus0, Sigmas = Sigmas0, phis = phis0) (theta.final = EM(theta0)) ## Iteration: 1 ll differnece: 27206.85 ## Iteration: 2 ll differnece: 195.7352 ## Iteration: 3 ll differnece: 124.3022 ## Iteration: 4 ll differnece: 43.09508 ## Iteration: 5 ll differnece: 14.00595 ## Iteration: 6 ll differnece: 9.720739 ## Iteration: 7 ll differnece: 10.52907 ## Iteration: 8 ll differnece: 13.34927 ## Iteration: 9 ll differnece: 18.1401 ## Iteration: 10 ll differnece: 23.50497 ## Iteration: 11 ll differnece: 27.55922 ## Iteration: 12 ll differnece: 28.71809 ## Iteration: 13 ll differnece: 25.69147 ## Iteration: 14 ll differnece: 20.10685 ## Iteration: 15 ll differnece: 15.67748 ## Iteration: 16 ll differnece: 14.31082 ## Iteration: 17 ll differnece: 15.47378 ## Iteration: 18 ll differnece: 18.33269 ## Iteration: 19 ll differnece: 22.13416 ## Iteration: 20 ll differnece: 25.82935 ## Iteration: 21 ll differnece: 27.74146 ## Iteration: 22 ll differnece: 25.48004 ## Iteration: 23 ll differnece: 18.18744 ## Iteration: 24 ll differnece: 9.540094 ## Iteration: 25 ll differnece: 3.847487 ## Iteration: 26 ll differnece: 1.322018 ## Iteration: 27 ll differnece: 0.4200287 ## Iteration: 28 ll differnece: 0.1289125 ## Iteration: 29 ll differnece: 0.03897584 ## $mus ## $mus[[1]] ## [1] -0.03534303 -1.99996843 ## ## $mus[[2]] ## [1] 2.011046 2.027221 ## ## $mus[[3]] ## [1] -2.007137 2.070099 ## ## ## $Sigmas ## $Sigmas[[1]] ## [,1] [,2] ## [1,] 0.9973681 -0.0912405 ## [2,] -0.0912405 1.0528290 ## ## $Sigmas[[2]] ## [,1] [,2] ## [1,] 0.75036568 0.04210181 ## [2,] 0.04210181 0.77091873 ## ## $Sigmas[[3]] ## [,1] [,2] ## [1,] 0.71945002 -0.02774193 ## [2,] -0.02774193 0.89936925 ## ## ## $phis ## [1] 0.1734057 0.3432008 0.4833934 4.5 References: Expectation-Maximization for GMMs explained Youtube video: EM - GMM Example "],["application-2-hidden-markov-model.html", "Topic 5 Application 2: Hidden Markov Model 5.1 Overview of Hidden Markov Model 5.2 Baum-Welch Algorithm 5.3 Implementation in R", " Topic 5 Application 2: Hidden Markov Model 5.1 Overview of Hidden Markov Model 5.1.1 Markov Chain Markov Chain Intuition Before formally defining what a Markov chain is, consider this: Imagine you’re a weather forecaster and you want to predict whether it will be sunny, cloudy, or rainy tomorrow based on the weather conditions of the past few days. Let’s assume that the probability of tomorrow’s weather only depends on today’s weather and not on the weather before that. Suppose that, if it’s sunny today, there is a 60% chance of it being sunny tomorrow, a 30% chance of it being cloudy, and a 10% chance of it being rainy. \\[ \\begin{align*} &amp;\\mathbb{P}(\\text{sunny tmr}\\mid \\text{sunny today}) = 0.6 \\\\ &amp;\\mathbb{P}(\\text{cloudy tmr}\\mid \\text{sunny today}) = 0.3 \\\\ &amp;\\mathbb{P}(\\text{rainy tmr}\\mid \\text{sunny today}) = 0.1 \\\\ \\end{align*} \\] If it’s cloudy today, there is a 20% chance of it being sunny tomorrow, a 70% chance of it still being cloudy, and a 10% chance of it being rainy. \\[ \\begin{align*} &amp;\\mathbb{P}(\\text{sunny tmr}\\mid \\text{cloudy today}) = 0.2 \\\\ &amp;\\mathbb{P}(\\text{cloudy tmr}\\mid \\text{cloudy today}) = 0.7 \\\\ &amp;\\mathbb{P}(\\text{rainy tmr}\\mid \\text{cloudy today}) = 0.1 \\\\ \\end{align*} \\] If it’s rainy today, there is a 30% chance of it being sunny tomorrow, a 40% chance of it still being cloudy, and a 30% chance of it being rainy. \\[ \\begin{align*} &amp;\\mathbb{P}(\\text{sunny tmr}\\mid \\text{rainy today}) = 0.3 \\\\ &amp;\\mathbb{P}(\\text{cloudy tmr}\\mid \\text{rainy today}) = 0.4 \\\\ &amp;\\mathbb{P}(\\text{rainy tmr}\\mid \\text{rainy today}) = 0.3 \\\\ \\end{align*} \\] In informal “mathematical” notation, we can represent this as: \\[ \\text{Weather States} = \\{\\text{Sunny}, \\text{Cloudy}, \\text{Rainy}\\} \\] \\[ \\text{Transition Probabilities Matrix} = \\begin{bmatrix} 0.6 &amp; 0.3 &amp; 0.1 \\\\ 0.2 &amp; 0.7 &amp; 0.1 \\\\ 0.3 &amp; 0.4 &amp; 0.3 \\end{bmatrix} \\] In this case (and with our assumptions above), the sequence of the weather states overtime, say {Sunny, Sunny, Sunny, Cloudy, Cloudy, Rainy, Cloudy,…}, is a classic Markov chain. This is because the weather tomorrow only depends on the weather today, or more broadly, the state at time \\(t+1\\) only depends on the state at time \\(t\\). Markov Chain Definition Mathematically, we can denote a Markov chain by \\[ X=(X_n)_{n\\in\\mathbb{N}} = (X_0, X_1, X_2,...) \\] where at each timestamp the process takes its values in a discrete set \\(E\\) such that \\[ X_n \\in E \\quad \\forall n\\in \\mathbb{N} \\] Then, the Markov property implies that we have \\[ \\mathbb{P}(X_{n+1}=s_{n+1}\\mid X_n = s_n, X_{n-1}=s_{n-1},X_{n-2}=s_{n-2},\\cdots) = \\mathbb{P}(X_{n+1}=s_{n+1}\\mid X_n = s_n) \\] 5.1.2 Hidden Markov Model Hidden Markov Model Intuition To illustrate Hidden Markov Model, we can continue with the weather example. Let’s assume that we are observing whether the grass is wet or dry every day, but we don’t directly observe the weather. That being said, we only observe the effect of the weather on the grass, and we assume whether the grass is wet or dry only depends on the weather that day. \\[ \\text{Weather States} = \\{\\text{Sunny}, \\text{Cloudy}, \\text{Rainy}\\} \\] \\[ \\text{Grass States} = \\{\\text{Dry}, \\text{Wet}\\} \\] Suppose that, if it’s sunny today, there is a \\(100 \\%\\) chance of observing dry grass, a \\(0 \\%\\) chance of observing wet grass. \\[ \\mathbb{P}(\\text{dry grass}\\mid \\text{Sunny}) = 1 \\\\ \\mathbb{P}(\\text{wet grass}\\mid \\text{Sunny}) = 0 \\] Suppose that, if it’s sunny today, there is a \\(70 \\%\\) chance of observing dry grass, a \\(30 \\%\\) chance of observing wet grass. \\[ \\mathbb{P}(\\text{dry grass}\\mid \\text{Cloudy}) = 0.7 \\\\ \\mathbb{P}(\\text{wet grass}\\mid \\text{Cloudy}) = 0.3 \\] Suppose that, if it’s rainy today, there is a \\(10 \\%\\) chance of observing dry grass, a \\(90 \\%\\) chance of observing wet grass. \\[ \\mathbb{P}(\\text{dry grass}\\mid \\text{Cloudy}) = 0.1 \\\\ \\mathbb{P}(\\text{wet grass}\\mid \\text{Cloudy}) = 0.9 \\] Aggregating the probabilities above into one matrix to get the emission probabilities matrix. \\[ \\text{Emission Probabilities Matrix} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0.7 &amp; 0.3 \\\\ 0.1 &amp; 0.9 \\end{bmatrix} \\] Hidden Markov Model Definition A Hidden Markov Model (HMM) is a statistical model that represents a sequence of observable events, \\[ Y = (Y_t)_{1\\leq t\\leq T} = (Y_1, Y_2, ..., Y_T) \\] ,where each observation \\(Y_t\\) is generated by an unknown or hidden state sequence that is a Markov chain \\[ X = (X_t)_{1\\leq t\\leq T} = (X_1, X_2, ..., X_T) \\] , which is not directly observed. Formally, an HMM is defined by the following parameters: Hidden states in sequence X take values in a finite set \\((h_1,h_2,...,h_N)\\). Observations in sequence Y take values in a finite set \\((o_1,o_2,...,o_M)\\). An initial probability distribution over the hidden states \\(X = (X_t)_{1\\leq t\\leq T}\\), which is given by \\(\\pi = (\\pi_1, \\pi_2, ..., \\pi_N)\\) such that \\(\\pi_i = \\mathbb{P}(X_1 = h_i)\\). A transition probability matrix \\(A_{N\\times N}\\) = {\\(a_{ij}\\)}, where entry \\(a_{ij}\\) represents the probability of transitioning from state \\(h_i\\) to state \\(h_j\\). \\[ a_{ij} = \\mathbb{P}(X_{t+1} = h_j \\mid X_{t} = h_i) \\] An emission probability matrix \\(B_{N\\times M}\\) = {\\(b_{j(k)}\\)}, where \\(b_{jk} = b_{j}(o_k)\\) represents the probability of observing the \\(o_k\\) given that the system is in state \\(h_j\\). \\[ b_{jk} = b_{j}(o_{k}) = \\mathbb{P}(Y_t = o_k \\mid X_t = h_j) \\] The goal of an HMM is to estimate the hidden state sequence X given the observed sequence Y, or to compute the likelihood of the observed sequence Y given the model parameters \\((\\pi, A, B)\\). In fact, we can describe a hidden markov model by \\(\\theta = (\\pi, A, B)\\). In the next section, we will apply EM algorithm to estimate parameters of HMM model, which has a specific name – Baum-Welch Algorithm. 5.2 Baum-Welch Algorithm 5.2.1 Intuition The Baum-Welch algorithm finds a local maximum for \\(\\theta^{*} = \\text{argmax}_{\\theta}\\mathbb{P}(Y\\mid \\theta)\\). To put it in words, it finds the set of parameters \\(\\theta^{*}\\) that makes the observed sequence \\(Y\\) mostly likely to have occurred. 5.2.2 Algorithm in steps Step 1: Initial Guess Set \\(\\theta^{t=0} = (A^{t=0}, B^{t=0}, \\pi^{t=0})\\), where the guesses could be random or not depending on if no prior knowledge is available. Step 2: Forward-Backward Algorithm Let’s define some new variables for each observation in the sequence here – the forward probabilities and the backward probabilities. Definition of forward and backward probabilities For each observation \\(Y_{t} \\in Y\\), the forward probabilities, denoted as \\(\\alpha_t(i)\\), which represent the probability of \\(Y_{t}\\) being in state \\(o_i\\) at time \\(t\\) given the observations up to time \\(t\\), \\((Y_1, Y_2,...,Y_{t-1})\\). \\[ \\alpha_t(i) = \\mathbb{P}(Y_t = o_i\\mid Y_1 = y_1, Y_2 = y_2,...,Y_{t-1} = y_{t-1}) \\] , where \\(y = (y_t)_{0&lt;t\\leq T}\\) is the true observation sequence. On the other hand, the backward probabilities, denoted as \\(\\beta_t(i)\\), represent the probability of starting from state \\(o_i\\) at time \\(t\\) and generating the remaining observations from time \\(t+1\\) to the end of the sequence, \\((Y_{t+1},...,Y_T)\\). \\[ \\beta_t(i) = \\mathbb{P}(Y_t = o_i\\mid Y_{t+1} = y_{t+1}, Y_{t+2} = y_{t+2},...,Y_{T} = y_{T}) \\] Computation of forward and backward probabilities The forward probabilities can be computed using the following recursive formulas: \\[ \\begin{aligned} \\alpha_1(i) &amp;= \\pi_i \\cdot b_i(y_1), \\quad 1\\leq i \\leq N\\\\ \\alpha_t(i) &amp;= b_i(y_t) \\sum_{j=1}^{N} \\alpha_{t-1}(j) \\cdot a_{ji}, \\quad 1\\leq i \\leq N, \\quad 2 \\leq t \\leq T \\\\ \\end{aligned} \\] , where \\(N\\) is the number of states in the HMM, \\(a_{ij}\\) is the transition probability from state \\(h_i\\) to state \\(h_j\\), \\(b_i(y_t)\\) is the emission probability of observation \\(y_t\\) from state \\(h_i\\), and \\(y_t\\) is the true observation at time \\(t\\). The backward probabilities can be computed using similar recursive formulas: \\[ \\begin{aligned} \\beta_T(i) &amp;= 1, \\quad 1\\leq i \\leq N\\\\ \\beta_t(i) &amp;= \\sum_{j=1}^{N} a_{ij} \\cdot b_j(y_{t+1}) \\cdot \\beta_{t+1}(j), \\quad 1\\leq i \\leq N,\\quad 1 \\leq t \\leq T-1 \\end{aligned} \\] , where \\(N\\) is the number of states in the HMM, \\(a_{ij}\\) is the transition probability from state \\(h_i\\) to state \\(h_j\\), \\(b_i(y_t)\\) is the emission probability of observation \\(y_t\\) from state \\(h_i\\), and \\(y_t\\) is the true observation at time \\(t\\). Step 3: E-step Calculate two new sets of quantities: \\(\\gamma_t(i)\\) represents the probability of the hidden state at time \\(t\\), \\(X_t\\), being in state \\(h_i\\) given the entire observation sequence \\(Y=y\\) and parameter set \\(\\theta\\), and it can be calculated using Bayes’ theorem. \\[ \\begin{aligned} \\gamma_t(i) &amp;= \\mathbb{P}(X_t = h_i\\mid Y = y,\\theta) \\\\ &amp;= \\frac{\\mathbb{P}(X_t = h_i, Y = y \\mid \\theta)}{\\mathbb{P}(Y=y \\mid \\theta)} \\\\ &amp;= \\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{j=1}^{N}\\alpha_t(j)\\beta_t(j)} \\end{aligned} \\] \\(\\xi_t(i,j)\\) represents probability of the hidden state being in state \\(h_i\\) at time \\(t\\) and and being in state \\(h_j\\) at time \\(t+1\\) given the the entire observation sequence \\(Y=y\\) and parameter set \\(\\theta\\), and it can also be calculated using the Bayes’ Rule. \\[ \\begin{aligned} \\xi_t(i,j) &amp;= \\mathbb{P}(X_t=h_i, X_{t+1}=h_j\\mid Y=y,\\theta) \\\\ &amp;= \\frac{\\mathbb{P}(X_t = h_i, X_{t+1}=h_j,Y=y\\mid \\theta)}{\\mathbb{P}(Y=y\\mid \\theta)} \\\\ &amp;= \\frac{\\alpha_t(i)a_{ij}\\beta_{t+1}(j)b_j(y_{t+1})}{\\sum_{k=1}^{N}\\sum_{w=1}^{N}\\alpha_t(k)a_{kw}\\beta_{t+1}(w)b_w(y_{t+1})} \\end{aligned} \\] Notice here, that the denominators of \\(\\gamma_t(i)\\) and \\(\\xi_t(i,j)\\) are the same, and it calculates the probability of observing the sequence \\(y\\) that we observed under our current guess for \\(\\theta\\). Step 4: M-step Now, we can update the parameters of the hidden markov model. First, let’s update the initial probability distribution \\(\\pi = (\\pi_1, \\pi_2, ..., \\pi_N)\\). \\[ \\begin{aligned} \\hat{\\pi}_i &amp;= \\gamma_1(i) \\\\ &amp;= \\mathbb{P}(X_1 = h_i) \\\\ \\\\ \\hat{\\pi} &amp;= (\\gamma_1(1), \\gamma_1(2),...,\\gamma_1(N)) \\\\ &amp;= (\\mathbb{P}(X_1 = h_1), \\mathbb{P}(X_1 = h_2),...,\\mathbb{P}(X_1 = h_N)) \\\\ \\end{aligned} \\] Next, update the \\(a_{ij}\\) entry in the transition matrix \\(A\\) with the expected number of transitions from state \\(h_i\\) to state \\(h_j\\) divided by the expected total number of transitions from state \\(h_i\\) including transitions to itself which equals to the number of times state \\(h_i\\) appears in the observed sequence \\(y\\). \\[ \\hat{a}_{ij} = \\frac{\\sum_{t=1}^{T-1}\\xi_{t}(i,j)}{\\sum_{t=1}^{T-1}\\gamma_{t}(i)} \\] Last but not least, update \\(b_{ij}\\) entry in the emission matrix \\(B\\) with the expected number of times the output observations have been equal to \\(o_j\\) under hidden state \\(h_i\\) over the expected total number of \\(h_i\\) in the hidden states sequence \\(X\\). \\[ \\hat{b}_i(o_k) = \\frac{\\sum_{t=1}^{T}1_{y_t = o_k}\\gamma_i(t)}{\\sum_{t=1}^{T}\\gamma_i(t)} \\] , where \\[ 1_{y_t = o_k} = \\Bigg\\{^{ 1 \\text{ if y_t = o_k}}_{0 \\text{ otherwise}} \\] Step 5: Determine convergence status and repeat iteratively if needed Set a criteria for whether the estimation sequence is converging and our current guess is close enough to last guess. If not, then repeat from step 2 to step 5 iteratively until convergence. Alternatively, we can set a maximum number of iteration for the Baum-Welch algorithm, n. Once we have completed n iterations, the algorithm stops and we take the estimates from the last iteration as results. 5.3 Implementation in R 5.3.1 HMM package HMM package documentation: https://cran.r-project.org/web/packages/HMM/HMM.pdf #install.packages(&#39;HMM&#39;) library(HMM) Example usage of ‘baumWelch’ function in the HMM package: # Initialize HMM with states values sets, transition probabilities, and emission probabilities hmm = initHMM(c(&quot;A&quot;,&quot;B&quot;) # hidden states A and B ,c(&quot;L&quot;,&quot;R&quot;) # observation states L or R ,startProbs = c(0.5,0.5) ,transProbs=matrix(c(.9,.1,.1,.9),2) ,emissionProbs=matrix(c(.5,.51,.5,.49),2)) # print the HMM we just built to check print(hmm) ## $States ## [1] &quot;A&quot; &quot;B&quot; ## ## $Symbols ## [1] &quot;L&quot; &quot;R&quot; ## ## $startProbs ## A B ## 0.5 0.5 ## ## $transProbs ## to ## from A B ## A 0.9 0.1 ## B 0.1 0.9 ## ## $emissionProbs ## symbols ## states L R ## A 0.50 0.50 ## B 0.51 0.49 # Sequence of observation a = sample(c(rep(&quot;L&quot;,100),rep(&quot;R&quot;,300))) b = sample(c(rep(&quot;L&quot;,300),rep(&quot;R&quot;,100))) observation = c(a,b) # append vector b after vector a # Baum-Welch bw = baumWelch(hmm,observation, maxIteration = 10) # Input: hmm, observation, maxIteration (the maximum number of iterations in the Baum-Welch algorithm) print(bw$hmm) ## $States ## [1] &quot;A&quot; &quot;B&quot; ## ## $Symbols ## [1] &quot;L&quot; &quot;R&quot; ## ## $startProbs ## A B ## 0.5 0.5 ## ## $transProbs ## to ## from A B ## A 9.974699e-01 0.00253015 ## B 6.670293e-06 0.99999333 ## ## $emissionProbs ## symbols ## states L R ## A 0.2464713 0.7535287 ## B 0.7486563 0.2513437 Notice here that one drawback of the baumWelch function in the HMM package is that it treats the initial probability vector we put in as true initial probability rather than our random guess. Thus, it does not try to update initial probability, which lower the accuracy of the estimation when the initial probability vector we put in is not that accurate. 5.3.2 Revisit Weather-Grass Example Let’s do a simulation to test the performance of Baum-Welch algorithm here. # Initialize HMM with states values sets, transition probabilities, and emission probabilities hmm = initHMM(c(&quot;Sunny&quot;,&quot;Cloudy&quot;,&quot;Rainy&quot;), # hidden states: WEATHER * 3 c(&quot;Dry&quot;,&quot;Wet&quot;), # observation states: GRASS STATUS * 2 startProbs = c(1/3,1/3,1/3), transProbs = cbind(c(.6,.2,.3),c(.3,.4,.7),c(.1,.1,.3)), emissionProbs=rbind(c(1,0),c(.7,.3),c(.1,.9))) # print the HMM we just built to check print(hmm) ## $States ## [1] &quot;Sunny&quot; &quot;Cloudy&quot; &quot;Rainy&quot; ## ## $Symbols ## [1] &quot;Dry&quot; &quot;Wet&quot; ## ## $startProbs ## Sunny Cloudy Rainy ## 0.3333333 0.3333333 0.3333333 ## ## $transProbs ## to ## from Sunny Cloudy Rainy ## Sunny 0.6 0.3 0.1 ## Cloudy 0.2 0.4 0.1 ## Rainy 0.3 0.7 0.3 ## ## $emissionProbs ## symbols ## states Dry Wet ## Sunny 1.0 0.0 ## Cloudy 0.7 0.3 ## Rainy 0.1 0.9 # Simulate 5000 observations and corresponding hidden states consecutively sim = simHMM(hmm, 5000) # Baum-Welch bw = baumWelch(hmm,sim$observation, maxIteration = 10) print(bw$hmm) ## $States ## [1] &quot;Sunny&quot; &quot;Cloudy&quot; &quot;Rainy&quot; ## ## $Symbols ## [1] &quot;Dry&quot; &quot;Wet&quot; ## ## $startProbs ## Sunny Cloudy Rainy ## 0.3333333 0.3333333 0.3333333 ## ## $transProbs ## to ## from Sunny Cloudy Rainy ## Sunny 0.6481263 0.2120997 0.1397740 ## Cloudy 0.3535154 0.4445554 0.2019293 ## Rainy 0.2973958 0.4071881 0.2954161 ## ## $emissionProbs ## symbols ## states Dry Wet ## Sunny 1.0000000 0.0000000 ## Cloudy 0.7087220 0.2912780 ## Rainy 0.1058077 0.8941923 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
