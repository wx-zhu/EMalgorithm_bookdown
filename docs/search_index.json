[["index.html", "EM Algorithm Topic 1 Welcome!", " EM Algorithm Wenxuan Zhu, Jingyi Guan, Sarah Tannert-Lerner Spring 2023, MATH/STAT 455, Macalester College Topic 1 Welcome! "],["application-in-image-segmentation.html", "Topic 2 Application in Image Segmentation 2.1 Introduction 2.2 Latent variable 2.3 Why EM? 2.4 Example 2.5 Related Work", " Topic 2 Application in Image Segmentation 2.1 Introduction Image segmentation is the process of dividing an image into multiple segments or regions, each of which corresponds to a distinct object or part of the image. One of the main challenges in image segmentation is identifying the boundaries between different regions. EM algorithm can be used to segment the image into different regions based on their color, texture, or other properties. 2.2 Latent variable In image segmentation, the latent variable is the label that corresponds to each pixel in the image. We don’t know which label corresponds to each pixel, and we want to estimate the most likely label based on the observed properties of the pixels in the image. The observed data could be the color, texture, or other properties of each pixel. 2.3 Why EM? EM algorithm can be helpful in this case because it provides a way to estimate the most likely label for each pixel based on the observed properties of the pixels and the underlying statistical model. The algorithm works by iteratively estimating the probability of each pixel belonging to each label (the E-step) and then updating the model parameters to maximize the likelihood of the observed data given the estimated labels (the M-step). 2.4 Example One example of using EM algorithm for image segmentation is in medical imaging. In medical imaging, EM can be used to segment tumors or other abnormal structures from normal tissue. The observed data could be the intensity values of the pixels in the image, and the latent variable could be a binary label indicating whether each pixel belongs to the tumor or the normal tissue. EM algorithm can estimate the most likely label for each pixel based on the observed intensity values and the underlying statistical model. The resulting segmentation can then be used to assist in diagnosis or treatment planning. 2.5 Related Work There is a paper that applied EM algorithm to image segmentation that we think worth taking time reading. Warfield, Simon K., Kelly H. Zou, and William M. Wells. “Validation of image segmentation and expert quality with an expectation-maximization algorithm.” in Proceedings of Fifth International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), Part I. 2002. This paper presents a novel solution for accurately assessing and validating image segmentation algorithms, which is a persistent challenge in the medical image segmentation field. The authors utilizes the Expectation-Maximization algorithm to compute a probabilistic estimate of the “ground truth” segmentation from a group of expert segmentations and provides a measure of the quality of each expert, enabling direct comparison of expert and automated algorithm performance. This approach offers a significant advancement in the field of image segmentation and has the potential to improve the accuracy and precision of segmentation algorithms. "],["introduction-to-em.html", "Topic 3 Introduction to EM 3.1 Introduction 3.2 EM Algorithm Given Condition &amp; Limitation 3.3 EM Steps (5-step version) 3.4 EM Steps (2-key-step version) 3.5 References:", " Topic 3 Introduction to EM 3.1 Introduction Given condition: We have observed variables \\(X\\), but we believe (assume) that there are some “latent” variables \\(Z\\) that we cannot observe. Usually, when given we only have knowledge about observed variables \\(X\\), we may want to calculate the MLE for the parameter of interest \\(\\theta\\) in this case: \\[ \\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\log {L(\\theta)} \\] However, for some problems, it is hard to solve the above equation. → We use EM Algorithm instead! 3.2 EM Algorithm Given Condition &amp; Limitation Given condition: We have observed variables \\(X\\), but we believe (assume) that there are some “latent” variables *\\(Z\\) that we cannot observe. Assume some parameterized distribution for the latent variables \\(Z\\) or \\(p(Z\\mid X, \\theta)\\) Limitation: EM estimate is only guaranteed to never get worse, but won’t necessarily find the global optimum of the likelihood In practice, start EM from multiple initial guesses and choose the one with the largest likelihood as the final guess for \\(\\theta\\) 3.3 EM Steps (5-step version) EM algorithm is usually described as two steps (the E-step and the M-step), but we think it’s helpful to think of EM as five distinct steps: Pick an initial guess for \\(\\theta^{t = 0}\\) for the parameter of interest \\(\\theta\\) Given the observed variable \\(X\\), calculate the conditional distribution of the observed variable \\(X\\) and latent variable \\(Z\\): \\(p(X, Z \\mid \\theta^{t})\\) Find the Q-function, which is the expected \\(\\log {p(X,Z\\mid \\theta)}\\) by using the conditional distribution of latent variable \\(Z\\) from the last step \\[ \\begin{align} Q(\\theta \\mid \\theta^t) &amp; = expected\\; \\log {p(X,Z\\mid \\theta)} \\\\ &amp; = E_{Z\\mid x,\\theta^t} [\\log{p(X,Z\\mid \\theta)}] \\\\ &amp; = \\int^{all\\; Z} \\log{p(X,Z \\mid \\theta)p(Z\\mid X,\\theta^t)} \\,dZ \\end{align} \\] Find the new guess \\(\\theta^{t+1}\\) by maximizing Q-function above Let \\(t = t+1\\) and go back to Step 2 3.4 EM Steps (2-key-step version) We also include the E-step and M-step version of EM algorithm below for reference: Start with random guess for \\(\\theta\\) E step (finding the Q-function): \\(Q(\\theta \\mid \\theta^t) = E_{Z|X,\\theta^m} [\\log{p(X, Z \\mid \\theta)}]\\) M step (finding the new guess for \\(\\theta\\) by maximizing the Q-function): \\(\\theta^{t+1} = \\underset{\\theta}{\\operatorname{argmax}} \\: Q(\\theta \\mid \\theta^t)\\) Repeat until \\(|\\theta^{t+1} - \\theta^m| &lt; \\epsilon\\) 3.5 References: EM Demystified: An Expectation-Maximization Tutorial Youtube: Expectation Maximization - 1 - Theory "],["example-binomial-mixture-model.html", "Topic 4 Example: Binomial Mixture Model 4.1 Overview of Binomial Mixture Model 4.2 Flipping Coins Example Illustrated 4.3 Implementation in R 4.4 References:", " Topic 4 Example: Binomial Mixture Model 4.1 Overview of Binomial Mixture Model A binomial mixture model is a statistical model that allows for the possibility that the observed data is generated from a mixture of two or more binomial distributions. In other words, the model assumes that the data comes from two or more groups or sub-populations, each of which may have a different probability of success for a given binary outcome. For example, consider a study of the effectiveness of a new medication in treating a disease. The data may consist of the number of patients who respond positively to the medication and the number who do not. If there are two or more sub-populations of patients with different response rates, a binomial mixture model can be used to model the data. The binomial mixture model is a useful tool for modeling data that comes from multiple sub-populations with different characteristics. It has applications in many areas, including medicine, biology, marketing, and finance. 4.2 Flipping Coins Example Illustrated 4.2.1 Scenario Assume we have 2 biased coins indexed \\(z=0\\) and \\(z=1\\) with probability \\(p_0\\) and \\(p_1\\) of landing heads, respectively. Suppose we randomly pick one of the coins and flip it m times, and do this procedure for \\(n\\) trials, and we observe and record the \\(m × n\\) results: \\(X_{11}, ..., X_{mn}\\), where \\(X_{ij}\\) is the r.v. denoting the outcome of the \\(i^{th}\\) coin flip from the \\(j^{th}\\) trial, with \\(X_{ij} = 1\\) indicating heads and \\(X_{ij} = 0\\) indicating tails. Then, we have that \\[ X_i\\mid_{z=0} \\sim Bernoulli(p_0) \\] \\[ X_i\\mid_{z=1} \\sim Bernoulli(p_1) \\] Equivalently, \\[ \\sum_{i=1}^{m}X_i\\mid_{z=0} \\sim Binomial(m, p_0) \\] \\[ \\sum_{i=1}^{m}X_i\\mid_{z=1} \\sim Binomial(m, p_1) \\] If the sequence \\(\\{z_j\\}\\), where \\(j\\) indexing individual trials, is given, we can simply use MLE to estimate \\(p_0\\) and \\(p_1\\) that maximize the probability that we observed the outcomes that we observed. However, when the sequence \\(\\{z_j\\}\\) is unknown, we should use the EM algorithm! 4.2.2 Algorithm steps Following the steps we discussed in the previous video, the complete procedure of deriving the estimates can be summarized as follows. Step 1 Pick initial guess: \\(p_0^{(t=0)}\\) and \\(p_1^{(t=0)}\\). Step 2 Derive the joint pdf Recall that we denote the true probability of the coin indexed \\(z=0\\) landing heads to be \\(p_0\\) and the true probability of the coin indexed \\(z=1\\) landing heads to be \\(p_1\\). Assume \\(p_0 = p_0^{(t)}\\) and \\(p_1 = p_1^{(t)}\\). Get the joint pdf as follows. \\[ P(X_{11},\\cdots, X_{mn}, \\mathbf{z}\\mid p_0^{(t)}, p_1^{(t)}) = \\prod_{j=1}^{n}\\frac{1}{2}[{p_0^{(t)}}^{\\sum_{i=1}^{m}X_{ij}}{(1-p_0^{(t)})}^{m-\\sum_{i=1}^{m}X_{ij}}]^{1-z_j} [{p_1^{(t)}}^{\\sum_{i=1}^{m}X_{ij}}{(1-p_1^{(t)})}^{m-\\sum_{i=1}^{m}X_{ij}}]^{z_j} \\] Step 3 Get Q-function using the joint pdf we got in step 2. Let \\(\\mathbf{p} = (p_0, p_1)\\) and \\(\\mathbf{p}^t = (p_0^t, p_1^t)\\). Recall the Q-function defined as follow in this specific scenario. \\[ Q(p;p^t) = \\mathbb{E}_{\\mathbf{z}\\mid \\mathbf{x}, p^{t}}\\log p(\\mathbf{x},\\mathbf{z}\\mid \\mathbf{p}) \\] , where \\(z\\) denotes our missing data, \\(x\\) is the observations. \\[ \\begin{aligned} \\log{p(\\mathbf{x}, \\mathbf{z}\\mid p_0, p_1)} &amp;= n\\log{\\frac{1}{2}} +\\sum_{j=1}^{n}[(1-z_j)\\log{[p_0^{\\sum_{i=1}^{m}x_{ij}}(1-p_0)^{m-\\sum_{i=1}^{m}x_{ij}}]} \\\\ &amp;+ z_j \\log{[p_1^{\\sum_{i=1}^{m}x_{ij}}(1-p_1)^{m-\\sum_{i=1}^{m}x_{ij}}]}] \\end{aligned} \\] Ignore the constant \\(n\\log{\\frac{1}{2}}\\), then we have \\[ \\begin{aligned} \\log{p(\\mathbf{x}, \\mathbf{z}\\mid p_0, p_1)} \\approx \\sum_{j=1}^{n}[(1-z_j)\\log{[p_0^{\\sum_{i=1}^{m}x_{ij}}(1-p_0)^{m-\\sum_{i=1}^{m}x_{ij}}]} \\\\ + z_j \\log{[p_1^{\\sum_{i=1}^{m}x_{ij}}(1-p_1)^{m-\\sum_{i=1}^{m}x_{ij}}]}] \\end{aligned} \\] The Q-function can thus be written as follows. \\[ \\begin{aligned} Q(p;p^t) &amp;\\approx \\sum_{j=1}^{n}[(1-\\mathbb{E}(z_j))\\log{[p_0^{\\sum_{i=1}^{m}x_{ij}}(1-p_0)^{m-\\sum_{i=1}^{m}x_{ij}}]} \\\\ &amp;+ \\mathbb{E}(z_j)\\log{[p_1^{\\sum_{i=1}^{m}x_{ij}}(1-p_1)^{m-\\sum_{i=1}^{m}x_{ij}}]}] \\end{aligned} \\] Then, let’s calculated the expected value of \\(\\textbf{z}\\). \\[ \\begin{aligned} &amp;\\mathbb{E}_{\\mathbf{z}\\mid \\mathbf{x},\\mathbf{p}^{t}}[z_j] = p(z_j=1\\mid \\sum_{i=1}^{m}X_i, \\mathbf{p}^t) \\text{ , because } z_j \\sim Bernoulli(0.5) \\\\ &amp;= \\frac{p(\\sum_{i=1}^{m} X_i \\mid z_j=1, \\mathbf{p}^t)p(z_j=1)}{\\sum_{j=1}^{n} p(\\sum_{i=1}^{m} X_i \\mid z_j, \\mathbf{p}^t)p(z_j)} \\text{ , Bayes&#39; Rule } \\\\ &amp;= \\frac{p(\\sum_{i=1}^{m} X_i \\mid z_j=1, \\mathbf{p}^t)p(z_j=1)}{ p(\\sum_{i=1}^{m} X_i \\mid z_j=1, \\mathbf{p}^t)p(z_j=1)+p(\\sum_{i=1}^{m} X_i \\mid z_j=0, \\mathbf{p}^t)p(z_j=0)} \\\\ &amp;= \\frac{p(\\sum_{i=1}^{m} X_i \\mid z_j=1, \\mathbf{p}^t)p(z_j=1)}{ p_1^{\\sum_{i=1}^{m} X_i}(1-p_1)^{m-\\sum_{i=1}^{m} X_i}p(z_j=1)+p_0^{\\sum_{i=1}^{m} X_i}(1-p_0)^{m-\\sum_{i=1}^{m} X_i}p(z_j=0)} \\\\ &amp;\\text{ , because }\\sum_{i=1}^{m} X_i \\text{ follows binomial distribution}\\\\ \\end{aligned} \\] We have our initial guess or guess from last iteration for \\(p_0\\), \\(p_1\\), and we have observed \\(\\sum X_i, m\\), and we assumed that \\(P(z_j=1) = P(z_j=0) = \\frac{1}{2}\\). Thus, we can calculate \\(\\mathbb{E}(z_j)\\). Now, we take \\(\\mathbb{E}(z_j)\\) as known. Step 4 Make new guess \\(p_0^{t+1}\\) and \\(p_1^{t+1}\\). \\[ \\mathbf{p}^{t+1} = \\underset{\\mathbf{p}}{\\operatorname{argmax}} Q(\\mathbf{p};\\mathbf{p}^t) \\] To maximize the Q-function \\(Q(\\mathbf{p};\\mathbf{p}^{t})\\), we take the derivative of it and set it to zero. Since \\(\\mathbf{p}\\) contains \\(p_0\\) and \\(p_1\\), we need to take 2 derivatives. First, take the derivative of the Q-function with respect to \\(p_0\\) and set it to zero. \\[ \\begin{aligned} \\frac{d Q(\\mathbf{p};\\mathbf{p}^t)}{d p_0^t} &amp;= \\frac{d}{d p_0} \\sum_{j=1}^{n}[(1-\\mathbb{E}(z_j))\\log{[p_0^{ \\sum_{i=1}^{m}x_{ij}}(1-p_0)^{m- \\sum_{i=1}^{m}x_{ij}}]} \\\\ &amp; \\quad\\; + \\mathbb{E}(z_j) \\log{[p_1^{ \\sum_{i=1}^{m}x_{ij}}(1-p_1)^{m-\\sum_{i=1}^{m}x_{ij}}]}] \\\\ &amp;= \\frac{d}{d p_0} \\sum_{j=1}^{n}[(1-\\mathbb{E}(z_j))\\log{[p_0^{\\sum_{i=1}^{m}x_{ij}}(1-p_0)^{m-\\sum_{i=1}^{m}x_{ij}}]}]\\\\ &amp;= \\frac{d}{d p_0} \\sum_{j=1}^{n}[(1-\\mathbb{E}(z_j))(\\sum_{i=1}^{m}x_{ij}\\log{p_0} + (m-\\sum_{i=1}^{m}x_{ij})\\log(1-p_0)]\\\\ &amp;=\\sum_{j=1}^{n} (1-\\mathbb{E}(z_j))(\\frac{\\sum_{i=1}^{m} X_{ij}}{p_0} - \\frac{m-\\sum_{i=1}^{m}X_{ij}}{1-p_0})\\\\ &amp;\\stackrel{set}{=} 0 \\\\ \\end{aligned} \\] Solve the score function to get \\(\\hat{p}_0\\). \\[ \\begin{aligned} \\sum_{j=1}^{n} (1-\\mathbb{E}(z_j))(\\frac{\\sum_{i=1}^{m} X_{ij}}{p_0} - \\frac{m-\\sum_{i=1}^{m}X_{ij}}{1-p_0}) &amp;\\stackrel{set}{=} 0 \\\\ \\frac{\\sum_{j=1}^{n}((1-\\mathbb{E}(z_j))\\sum_{i=1}^{m}x_{ij})}{p_0} &amp;= \\frac{\\sum_{j=1}^{n}((1-\\mathbb{E}(z_j))(m-\\sum_{i=1}^{m}x_{ij}))}{1-p_0} \\\\ (1-p_0)\\sum_{j=1}^{n}((1-\\mathbb{E}(z_j))\\sum_{i=1}^{m}x_{ij}) &amp;= p_0\\sum_{j=1}^{n}((1-\\mathbb{E}(z_j))(m-\\sum_{i=1}^{m}x_{ij})) \\\\ \\sum_{j=1}^{n}((1-\\mathbb{E}(z_j))\\sum_{i=1}^{m}x_{ij}) - p_0\\sum_{j=1}^{n}((1-\\mathbb{E}(z_j))\\sum_{i=1}^{m}x_{ij}) &amp;= mp_0\\sum_{j=1}^{n}(1-\\mathbb{E}(z_j)) - p_0\\sum_{j=1}^{n}((1-\\mathbb{E}(z_j)\\sum_{i=1}^{m}x_{ij}) \\\\ \\sum_{j=1}^{n}((1-\\mathbb{E}(z_j))\\sum_{i=1}^{m}x_{ij}) &amp;= mp_0\\sum_{j=1}^{n}(1-\\mathbb{E}(z_j)) \\\\ \\hat{p}_0 &amp;= \\frac{\\sum_{j=1}^{n}((1-\\mathbb{E}(z_j))\\sum_{i=1}^{m}x_{ij})}{m\\sum_{j=1}^{n}(1-\\mathbb{E}(z_j))} \\end{aligned} \\] Next, take the derivative of the Q-function with respect to \\(p_1\\) and set it to zero. \\[ \\begin{aligned} \\frac{d Q(\\mathbf{p};\\mathbf{p}^t)}{d p_1^t} &amp;= \\frac{d}{d p_1} \\sum_{j=1}^{n}[(1-\\mathbb{E}(z_j))\\log{[p_0^{ \\sum_{i=1}^{m}x_{ij}}(1-p_0)^{m- \\sum_{i=1}^{m}x_{ij}}]} \\\\ &amp; \\quad\\; + \\mathbb{E}(z_j) \\log{[p_1^{ \\sum_{i=1}^{m}x_{ij}}(1-p_1)^{m-\\sum_{i=1}^{m}x_{ij}}]}] \\\\ &amp;= \\frac{d}{d p_1} \\sum_{j=1}^{n}[\\mathbb{E}(z_j)\\log{[p_1^{\\sum_{i=1}^{m}x_{ij}}(1-p_1)^{m-\\sum_{i=1}^{m}x_{ij}}]}]\\\\ &amp;= \\frac{d}{d p_1} \\sum_{j=1}^{n}[\\mathbb{E}(z_j)(\\sum_{i=1}^{m}x_{ij}\\log{p_1} + (m-\\sum_{i=1}^{m}x_{ij})\\log(1-p_1)] \\\\ &amp;=\\sum_{j=1}^{n} \\mathbb{E}(z_j)(\\frac{\\sum_{i=1}^{m} x_{ij}}{p_1} - \\frac{m-\\sum_{i=1}^{m}x_{ij}}{1-p_1})\\\\ &amp;\\stackrel{set}{=} 0 \\\\ \\end{aligned} \\] Solve the score function to get \\(\\hat{p}_1\\). \\[ \\begin{aligned} \\sum_{j=1}^{n} \\mathbb{E}(z_j)(\\frac{\\sum_{i=1}^{m} x_{ij}}{p_1} - \\frac{m-\\sum_{i=1}^{m}x_{ij}}{1-p_1}) &amp;\\stackrel{set}{=} 0 \\\\ \\frac{\\sum_{j=1}^{n}\\mathbb{E}(z_j)\\sum_{i=1}^{m} x_{ij}}{p_1} &amp;= \\frac{\\sum_{j=1}^{n}\\mathbb{E}(z_j)(m-\\sum_{i=1}^{m}x_{ij})}{1-p_1} \\\\ (1-p_1)\\sum_{j=1}^{n}\\mathbb{E}(z_j)\\sum_{i=1}^{m} x_{ij} &amp;= p_1\\sum_{j=1}^{n}\\mathbb{E}(z_j)(m-\\sum_{i=1}^{m}x_{ij}) \\\\ \\sum_{j=1}^{n}\\mathbb{E}(z_j)\\sum_{i=1}^{m} x_{ij} - p_1\\sum_{j=1}^{n}\\mathbb{E}(z_j)\\sum_{i=1}^{m} x_{ij} &amp;= mp_1\\sum_{j=1}^{n}\\mathbb{E}(z_j) - \\sum_{j=1}^{n}\\mathbb{E}(z_j)\\sum_{i=1}^{m}x_{ij}\\\\ \\sum_{j=1}^{n}\\mathbb{E}(z_j)\\sum_{i=1}^{m} x_{ij} &amp;= mp_1\\sum_{j=1}^{n}\\mathbb{E}(z_j) \\\\ \\hat{p}_1 = \\frac{\\sum_{j=1}^{n}\\mathbb{E}(z_j)\\sum_{i=1}^{m} x_{ij}}{m\\sum_{j=1}^{n}\\mathbb{E}(z_j)} \\end{aligned} \\] We set \\(p_0^{t+1} = \\hat{p}_0\\) and \\(p_1^{t+1} = \\hat{p}_1\\) to let those estimates be our estimated results for this new iteration. Step 5 Condition to stop otherwise keep iterating If it reaches numerical precision when comparing \\(\\textbf{p}^{t}\\) and \\(\\textbf{p}^{t+1}\\), we stop the procedure and take the estimates from the last iteration as our results. Otherwise, let \\(t=t+1\\), and repeat step 2-5 until it reaches numerical precision. 4.3 Implementation in R 4.3.1 Simplified case (2-coin) Set up set.seed(455) K = 2 # number of coins we have p = runif(K) # the probability of each coin to fall on Heads (TRUE PARAMETER!) n = 1000 # number of trials m = 10 # number of flips per trial # Note here that we are using one single coin for a whole trial, we never change coins within one trial # Latent - coin sequence - sequence with 0 and 1, where 0 on the ith spot means we are using the z = 0 coin on the ith trial (z = rbinom(n,1,0.5)) ## [1] 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 ## [35] 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 ## [69] 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 ## [103] 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 ## [137] 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 ## [171] 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 ## [205] 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 ## [239] 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 ## [273] 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 ## [307] 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 ## [341] 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 ## [375] 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 ## [409] 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 ## [443] 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 ## [477] 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 ## [511] 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 ## [545] 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 ## [579] 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 ## [613] 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 1 ## [647] 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 ## [681] 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 ## [715] 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 ## [749] 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 ## [783] 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 ## [817] 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 ## [851] 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 ## [885] 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 ## [919] 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 ## [953] 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 ## [987] 0 1 0 1 1 1 0 1 0 0 1 1 0 0 # Observed - simulating the observed head/tail for individual flips based on the true probability of each coin to fall on Heads/tails x = matrix(nrow = n, ncol = m) # Fill in the observation matrix for(j in 1:n){ # for each trial pj = p[z[j]+1] # assign the corresponding probability of landing heads to pi # Note: Why do we need to plus one here? # The probability of landing heads for coin z=0 is stored at p[1] # The probability of landing heads for coin z=1 is stored at p[2] x[j,] = rbinom(m, 1, pj) # simulate the flips in an individual trial, with 1 denoting head and 0 denoting tail } Estimation 1st step: pick initial guess of \\(p_0^{t=0}\\) and \\(p_1^{t=0}\\) # Let&#39;s just let R decide our initial guess here! (initialp = runif(2)) ## [1] 0.2650827 0.7531283 2nd step: Get the likelihood function of the complete data assuming our current guesses are the true value of the probabilities of landing heads \\[ \\mathbb{P}(X_{11},...,X_{mn}, \\textbf{z}\\mid p_0^{(t)}, p_1^{(t)}) = \\prod_{j=1}^{n}\\frac{1}{2}[{p_0^{(t)}}^{\\sum_{i=1}^{m}X_{ij}}{(1-p_0^{(t)})}^{m-\\sum_{i=1}^{m}X_{ij}}]^{1-z_j} [{p_1^{(t)}}^{\\sum_{i=1}^{m}X_{ij}}{(1-p_1^{(t)})}^{m-\\sum_{i=1}^{m}X_{ij}}]^{z_j} \\] 3rd step: get the Q-function \\[ Q(p;p^t) = \\mathbb{E}_{\\mathbf{z}\\mid \\mathbf{x}, p^{t}}\\log p(\\mathbf{x},\\mathbf{z}\\mid \\mathbf{p}) \\] \\[ \\begin{aligned} Q(p;p^t) &amp;\\approx \\sum_{j=1}^{n}[(1-\\mathbb{E}(z_j))\\log{[p_0^{\\sum_{i=1}^{m}x_{ij}}(1-p_0)^{m-\\sum_{i=1}^{m}x_{ij}}]} \\\\ &amp;+ \\mathbb{E}(z_j)\\log{[p_1^{\\sum_{i=1}^{m}x_{ij}}(1-p_1)^{m-\\sum_{i=1}^{m}x_{ij}}]}] \\end{aligned} \\] In the video, we mentioned that we are able to calculate the expected value of z for each trial, where z denotes which coin we use for that trial. We calculate it here for later use. \\[ \\begin{aligned} &amp;\\mathbb{E}_{\\mathbf{z}\\mid \\mathbf{x},\\mathbf{p}^{t}}[z_j] = p(z_j=1\\mid \\sum_{i=1}^{m}X_i, \\mathbf{p}^t) \\text{ , because } z_j \\sim Bernoulli(0.5) \\\\ &amp;= \\frac{p(\\sum_{i=1}^{m} X_i \\mid z_j=1, \\mathbf{p}^t)p(z_j=1)}{\\sum_{j=1}^{n} p(\\sum_{i=1}^{m} X_i \\mid z_j, \\mathbf{p}^t)p(z_j)} \\text{ , Bayes&#39; Rule } \\\\ &amp;= \\frac{p(\\sum_{i=1}^{m} X_i \\mid z_j=1, \\mathbf{p}^t)p(z_j=1)}{ p(\\sum_{i=1}^{m} X_i \\mid z_j=1, \\mathbf{p}^t)p(z_j=1)+p(\\sum_{i=1}^{m} X_i \\mid z_j=0, \\mathbf{p}^t)p(z_j=0)} \\\\ &amp;= \\frac{p(\\sum_{i=1}^{m} X_i \\mid z_j=1, \\mathbf{p}^t)p(z_j=1)}{ p_1^{\\sum_{i=1}^{m} X_i}(1-p_1)^{m-\\sum_{i=1}^{m} X_i}p(z_j=1)+p_0^{\\sum_{i=1}^{m} X_i}(1-p_0)^{m-\\sum_{i=1}^{m} X_i}p(z_j=0)} \\\\ &amp;\\text{ , because }\\sum_{i=1}^{m} X_i \\text{ follows binomial distribution} \\end{aligned} \\] # Calculate the expected value of z for each trial getExpectedZ = function(p){ E = rep(NA,n) # initialize a new vector for storing expected sequence of z, since we have n trials, the length of the sequence should be set to n for(j in 1:n){ trialsum = sum(x[j,]) # get the flip outcomes from the ith trial, store them as a vector named xi n1 = (p[2] ^ trialsum) * ((1 - p[2]) ^ (m - trialsum)) * (1/2) # probability of observing the number of heads we observed when we use coin z = 1 n0 = (p[1] ^ trialsum) * ((1 - p[1]) ^ (m - trialsum)) * (1/2) # probability of observing the number of heads we observed when we use coin z = 0 E[j] = n1/(n1+n0) # expected value of z at the jth trial } return(E) # expected sequence of z } 4th step: Make new guess for \\(p_0^{t+1}\\) and p_1^{t+1} Recall that we derived: \\[ p_0^{t+1} = \\frac{\\sum_{j=1}^{n}(1-\\mathbb{E}(z_j))\\sum_{i=1}^{m}X_{ij}}{m\\sum_{j=1}^{n}(1-\\mathbb{E}(z_j))} \\] \\[ p_1^{t+1} = \\frac{\\sum_{j=1}^{n}\\mathbb{E}(z_j)\\sum_{i=1}^{m}X_{ij}}{m\\sum_{j=1}^{n}(\\mathbb{E}(z_j)} \\] getNewGuess = function(E){ p=rep(NA, K) # initialize a new vector for storing expected sequence of z, since we have two coins, the length of the s equence should be set to 2, which is K trialsums = rowSums(x) # get the sum of flip outcomes for each trial, store them as a vector named trialsums # Note: suppose we conducted 2 trials and we observed 4 heads in the first trial and 5 heads in the second, trialsums = (4,5) p[1] = sum((1-E)*trialsums)/(m*sum(1-E)) # calculate new estimate for the probability of landing heads for coin z = 0 p[2] = sum(E*trialsums)/(m*sum(E)) # calculate new estimate for the probability of landing heads for coin z = 1 return(p) # return the new set of estimates for this iteration } 5th step: create the iterative function with a deciding mechanism for whether we continue the iteration process or stop # EM EM = function(initialguess, maxIter=50, tol=1e-5){ # we set default values to maxIter and tol, but try out different values here to see what happens. pt=initialguess for(i in 1:maxIter){ E = getExpectedZ(pt) # the E step pt = getNewGuess(E) # the M step cat(&quot;Iteration: &quot;, i, &quot;pt: &quot;,pt,&quot;\\n&quot;) print(pt) # print the new set of estimates of parameters for every iteration if(norm(pt-p,type=&quot;2&quot;) &lt; tol) break # if the new estimates are similar enough to the estimates from the last iteration, we stop the process and take the estimates we get from the new iteration as our final estimates p = pt # Otherwise, we regard this new set of estimates as &quot;previous&quot; and get new estimates based on that } return(pt) } Put our initial guess for p into the function EM to get the EM estimates of \\(p_0\\) and \\(p_1\\): sort(EM(initialp)) ## Iteration: 1 pt: 0.4740147 0.8322268 ## [1] 0.4740147 0.8322268 ## Iteration: 2 pt: 0.5761333 0.8824783 ## [1] 0.5761333 0.8824783 ## Iteration: 3 pt: 0.6197177 0.9098317 ## [1] 0.6197177 0.9098317 ## Iteration: 4 pt: 0.6391757 0.9240014 ## [1] 0.6391757 0.9240014 ## Iteration: 5 pt: 0.648519 0.9315073 ## [1] 0.6485190 0.9315073 ## Iteration: 6 pt: 0.6533188 0.935579 ## [1] 0.6533188 0.9355790 ## Iteration: 7 pt: 0.6559 0.9378264 ## [1] 0.6559000 0.9378264 ## Iteration: 8 pt: 0.6573252 0.9390812 ## [1] 0.6573252 0.9390812 ## Iteration: 9 pt: 0.658123 0.939787 ## [1] 0.658123 0.939787 ## Iteration: 10 pt: 0.658573 0.9401857 ## [1] 0.6585730 0.9401857 ## Iteration: 11 pt: 0.6588277 0.9404117 ## [1] 0.6588277 0.9404117 ## Iteration: 12 pt: 0.6589721 0.9405398 ## [1] 0.6589721 0.9405398 ## Iteration: 13 pt: 0.6590542 0.9406126 ## [1] 0.6590542 0.9406126 ## Iteration: 14 pt: 0.6591008 0.940654 ## [1] 0.6591008 0.9406540 ## Iteration: 15 pt: 0.6591273 0.9406775 ## [1] 0.6591273 0.9406775 ## Iteration: 16 pt: 0.6591423 0.9406909 ## [1] 0.6591423 0.9406909 ## Iteration: 17 pt: 0.6591509 0.9406985 ## [1] 0.6591509 0.9406985 ## Iteration: 18 pt: 0.6591558 0.9407028 ## [1] 0.6591558 0.9407028 ## [1] 0.6591558 0.9407028 Check the TRUE PARAMETERS that we set at the very start: sort(p) ## [1] 0.6563497 0.9495199 Comparing our EM estimates to the true parameters here, we think they are reasonablly close enough to each other. 4.3.2 Generalized case (k-coin) #install.packages(&quot;extraDistr&quot;) # uncommented this line to install the package if you haven&#39;t already library(extraDistr) K = 5 # set the number of distinct coins, try different numbers of coins here to see what happens:) Kplus1 = K + 1 p = seq(1/Kplus1, 1-1/Kplus1, 1/Kplus1) # the probability of each coin to fall on Heads # the intuition here is we want to scatter their probabilities of landing heads as far enough as possible between 0 and 1 n = 1000 m = 100 # Latent z = rcat(n, rep(1/K, K)) # generating a sequence of length n, with a probability of 1/K of choosing any integer from 1 to K for each of the n spots # Observed ## Create an empty matrix with n rows and m columns x = matrix(nrow = n, ncol = m) ## Fill in the matrix with simulated observations for(j in 1:n){ pj = p[z[j]] x[j,] = rbinom(m, 1, pj) } # The getExpectedZ step is conventionally called the E-step, which refers to Expectation-step doE = function(p){ E = matrix(nrow = n, ncol = K) for(j in 1:n){ for(k in 1:K){ xj = x[j,] E[j,k] = p[k]^sum(xj) + (1-p[k])^sum(1-xj) } E[j,] = E[j,]/sum(E[j,]) # Scale rows of the matrix to make rowSums one } return(E) } # the getNewGuess step is conventionally called the M-step, which refers to Maximization-step doM = function(E){ p = rep(NA, K) xi = rowSums(x) p = colSums(E*xi)/(m*colSums(E)) # analogous to the 2-coin example, we will take this formula as known and use it directly here return(p) } # The following is exactly the same as in the 2-coin example # EM step EM = function(initialguess, maxIter = 100, tol = 1e-5){ pt = initialguess for(i in 1:maxIter){ E = doE(pt) pt = doM(E) cat(&quot;Iteration: &quot;, i, &quot;pt: &quot;, pt, &quot;\\n&quot;) if(norm(pt-p, type=&quot;2&quot;) &lt; tol) break p = pt } return(pt) } # set initial guess initialp = runif(K) p.final = sort(EM(initialp)) ## Iteration: 1 pt: 0.8565236 0.792371 0.6068438 0.8373582 0.210492 ## Iteration: 2 pt: 0.3635598 0.2192907 0.1745154 0.3115444 0.7601974 ## Iteration: 3 pt: 0.8570847 0.7535435 0.649648 0.8458564 0.244055 ## Iteration: 4 pt: 0.380842 0.1769044 0.1488555 0.3440218 0.7814052 ## Iteration: 5 pt: 0.8558956 0.7145043 0.6410025 0.8616165 0.2369488 ## Iteration: 6 pt: 0.3557277 0.1552665 0.1496867 0.3749354 0.7833758 ## Iteration: 7 pt: 0.8625758 0.6659304 0.6502534 0.8581712 0.2346125 ## Iteration: 8 pt: 0.3722803 0.1451943 0.1461729 0.3574372 0.7834335 ## Iteration: 9 pt: 0.8588625 0.6500157 0.6528877 0.8627856 0.229711 ## Iteration: 10 pt: 0.3556754 0.1455972 0.1451964 0.3686685 0.7808668 ## Iteration: 11 pt: 0.8627724 0.650215 0.6490227 0.859834 0.2281329 ## Iteration: 12 pt: 0.3666581 0.1453665 0.1455651 0.3569591 0.7803168 ## Iteration: 13 pt: 0.8604307 0.6490601 0.6496518 0.8625591 0.2278975 ## Iteration: 14 pt: 0.3584471 0.1455247 0.1454233 0.3654753 0.7803241 ## Iteration: 15 pt: 0.8623429 0.6495233 0.6492214 0.8607907 0.2279081 ## Iteration: 16 pt: 0.364663 0.1454438 0.1454955 0.3595345 0.7803816 ## Iteration: 17 pt: 0.8610272 0.6493283 0.6494825 0.8621681 0.2279351 ## Iteration: 18 pt: 0.3602922 0.1454793 0.1454529 0.3640631 0.7804182 ## Iteration: 19 pt: 0.8620343 0.6494647 0.6493862 0.8611919 0.2279516 ## Iteration: 20 pt: 0.3636126 0.1454574 0.1454709 0.3608275 0.780438 ## Iteration: 21 pt: 0.8613095 0.6494163 0.6494563 0.8619331 0.2279605 ## Iteration: 22 pt: 0.361212 0.1454665 0.1454597 0.3632738 0.7804486 ## Iteration: 23 pt: 0.861857 0.6494523 0.6494319 0.8613947 0.2279653 ## Iteration: 24 pt: 0.3630194 0.1454608 0.1454643 0.3614911 0.7804543 ## Iteration: 25 pt: 0.8614569 0.6494401 0.6494504 0.8617997 0.2279679 ## Iteration: 26 pt: 0.3616952 0.1454631 0.1454614 0.3628289 0.7804574 ## Iteration: 27 pt: 0.8617569 0.6494496 0.6494443 0.8615024 0.2279693 ## Iteration: 28 pt: 0.3626864 0.1454616 0.1454625 0.3618451 0.7804591 ## Iteration: 29 pt: 0.861536 0.6494465 0.6494492 0.8617248 0.22797 ## Iteration: 30 pt: 0.3619556 0.1454622 0.1454618 0.36258 0.7804601 ## Iteration: 31 pt: 0.8617009 0.6494491 0.6494477 0.8615607 0.2279705 ## Iteration: 32 pt: 0.3625007 0.1454618 0.145462 0.3620372 0.7804606 ## Iteration: 33 pt: 0.861579 0.6494483 0.649449 0.8616831 0.2279707 ## Iteration: 34 pt: 0.3620975 0.145462 0.1454618 0.3624416 0.7804608 ## Iteration: 35 pt: 0.8616698 0.649449 0.6494487 0.8615925 0.2279708 ## Iteration: 36 pt: 0.3623976 0.1454619 0.1454619 0.3621422 0.780461 ## Iteration: 37 pt: 0.8616025 0.6494488 0.649449 0.8616599 0.2279709 ## Iteration: 38 pt: 0.3621753 0.1454619 0.1454619 0.3623649 0.7804611 ## Iteration: 39 pt: 0.8616526 0.649449 0.6494489 0.86161 0.2279709 ## Iteration: 40 pt: 0.3623406 0.1454619 0.1454619 0.3621998 0.7804611 ## Iteration: 41 pt: 0.8616155 0.649449 0.649449 0.8616471 0.227971 ## Iteration: 42 pt: 0.362218 0.1454619 0.1454619 0.3623225 0.7804612 ## Iteration: 43 pt: 0.861643 0.649449 0.649449 0.8616196 0.227971 ## Iteration: 44 pt: 0.3623091 0.1454619 0.1454619 0.3622315 0.7804612 ## Iteration: 45 pt: 0.8616226 0.649449 0.649449 0.86164 0.227971 ## Iteration: 46 pt: 0.3622415 0.1454619 0.1454619 0.3622991 0.7804612 ## Iteration: 47 pt: 0.8616378 0.649449 0.649449 0.8616248 0.227971 ## Iteration: 48 pt: 0.3622917 0.1454619 0.1454619 0.3622489 0.7804612 ## Iteration: 49 pt: 0.8616265 0.649449 0.649449 0.8616361 0.227971 ## Iteration: 50 pt: 0.3622544 0.1454619 0.1454619 0.3622862 0.7804612 ## Iteration: 51 pt: 0.8616349 0.649449 0.649449 0.8616277 0.227971 ## Iteration: 52 pt: 0.3622821 0.1454619 0.1454619 0.3622585 0.7804612 ## Iteration: 53 pt: 0.8616287 0.649449 0.649449 0.861634 0.227971 ## Iteration: 54 pt: 0.3622616 0.1454619 0.1454619 0.3622791 0.7804612 ## Iteration: 55 pt: 0.8616333 0.649449 0.649449 0.8616293 0.227971 ## Iteration: 56 pt: 0.3622768 0.1454619 0.1454619 0.3622638 0.7804612 ## Iteration: 57 pt: 0.8616298 0.649449 0.649449 0.8616328 0.227971 ## Iteration: 58 pt: 0.3622655 0.1454619 0.1454619 0.3622751 0.7804612 ## Iteration: 59 pt: 0.8616324 0.649449 0.649449 0.8616302 0.227971 ## Iteration: 60 pt: 0.3622739 0.1454619 0.1454619 0.3622667 0.7804612 ## Iteration: 61 pt: 0.8616305 0.649449 0.649449 0.8616321 0.227971 ## Iteration: 62 pt: 0.3622676 0.1454619 0.1454619 0.362273 0.7804612 ## Iteration: 63 pt: 0.8616319 0.649449 0.649449 0.8616307 0.227971 ## Iteration: 64 pt: 0.3622723 0.1454619 0.1454619 0.3622683 0.7804612 ## Iteration: 65 pt: 0.8616309 0.649449 0.649449 0.8616318 0.227971 ## Iteration: 66 pt: 0.3622688 0.1454619 0.1454619 0.3622718 0.7804612 ## Iteration: 67 pt: 0.8616316 0.649449 0.649449 0.861631 0.227971 ## Iteration: 68 pt: 0.3622714 0.1454619 0.1454619 0.3622692 0.7804612 ## Iteration: 69 pt: 0.8616311 0.649449 0.649449 0.8616316 0.227971 ## Iteration: 70 pt: 0.3622695 0.1454619 0.1454619 0.3622711 0.7804612 ## Iteration: 71 pt: 0.8616315 0.649449 0.649449 0.8616311 0.227971 ## Iteration: 72 pt: 0.3622709 0.1454619 0.1454619 0.3622697 0.7804612 ## Iteration: 73 pt: 0.8616312 0.649449 0.649449 0.8616314 0.227971 ## Iteration: 74 pt: 0.3622699 0.1454619 0.1454619 0.3622708 0.7804612 ## Iteration: 75 pt: 0.8616314 0.649449 0.649449 0.8616312 0.227971 ## Iteration: 76 pt: 0.3622706 0.1454619 0.1454619 0.36227 0.7804612 ## Iteration: 77 pt: 0.8616312 0.649449 0.649449 0.8616314 0.227971 ## Iteration: 78 pt: 0.3622701 0.1454619 0.1454619 0.3622706 0.7804612 ## Iteration: 79 pt: 0.8616314 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 80 pt: 0.3622705 0.1454619 0.1454619 0.3622701 0.7804612 ## Iteration: 81 pt: 0.8616313 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 82 pt: 0.3622702 0.1454619 0.1454619 0.3622704 0.7804612 ## Iteration: 83 pt: 0.8616313 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 84 pt: 0.3622704 0.1454619 0.1454619 0.3622702 0.7804612 ## Iteration: 85 pt: 0.8616313 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 86 pt: 0.3622702 0.1454619 0.1454619 0.3622704 0.7804612 ## Iteration: 87 pt: 0.8616313 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 88 pt: 0.3622704 0.1454619 0.1454619 0.3622703 0.7804612 ## Iteration: 89 pt: 0.8616313 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 90 pt: 0.3622703 0.1454619 0.1454619 0.3622703 0.7804612 ## Iteration: 91 pt: 0.8616313 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 92 pt: 0.3622703 0.1454619 0.1454619 0.3622703 0.7804612 ## Iteration: 93 pt: 0.8616313 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 94 pt: 0.3622703 0.1454619 0.1454619 0.3622703 0.7804612 ## Iteration: 95 pt: 0.8616313 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 96 pt: 0.3622703 0.1454619 0.1454619 0.3622703 0.7804612 ## Iteration: 97 pt: 0.8616313 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 98 pt: 0.3622703 0.1454619 0.1454619 0.3622703 0.7804612 ## Iteration: 99 pt: 0.8616313 0.649449 0.649449 0.8616313 0.227971 ## Iteration: 100 pt: 0.3622703 0.1454619 0.1454619 0.3622703 0.7804612 # print to see the TRUE parameters that we set at the beginning p ## [1] 0.1666667 0.3333333 0.5000000 0.6666667 0.8333333 # print to see our EM estimates for the parameters p.final ## [1] 0.1454619 0.1454619 0.3622703 0.3622703 0.7804612 Code from https://www.youtube.com/watch?v=J24CifymPbo 4.4 References: Expectation Maximization (EM) Algorithm - colorado.edu Youtube: Expectation Maximization - 2 - Example: Binomial Mixture Model "],["application-1-gaussian-mixture-model.html", "Topic 5 Application 1: Gaussian Mixture Model 5.1 Why using GMMs? 5.2 Train GMM using MLE 5.3 Train simple GMM example using EM 5.4 R code example 5.5 References:", " Topic 5 Application 1: Gaussian Mixture Model 5.1 Why using GMMs? Suppose we have a dataset with 2 features. When you plot your dataset, it might look like this: Figure 5.1: A dataset with 2 clusters, made by Maël Fabien From the above plot, there are 2 clusters. Thus, we are aiming to solve two tasks: Clustering of the data Modeling of the distribution for the data and the corresponding parameters. Usually, when doing clustering, people will think of k-means. But, in fact, K-means is just a special case for Gaussian Mixture Models (GMMs) when using a specific EM algorithm. The following plot shows some differences between K-means and GMMs for clustering. In general, for k-means, the clusters are defined by the data means whereas GMM, clusters are defined by data means and variance modeled as Gaussian (aka Normal distribution). Figure 5.2: K-Means vs. GMMs for clustering, made by Maël Fabien Thus, when using 2 Gaussians, we want to find: the mean and covariance of the first Gaussian the mean and covariance of the second Gaussian the weight of each Gaussian component 5.2 Train GMM using MLE When training a GMM, we want to identify a set of parameters that characterize our GMM. \\[ \\theta = (w_k, \\mu_k, \\Sigma_k), k = 1,2,3,\\dots, M \\] Similar to solve parameters in single Gaussian model using MLE, we can take the Score function \\(\\frac {dL}{d\\theta}ln L(\\theta)\\) using log-likelihood and set it equal to 0 again, where \\(\\prod^N_{i=1}\\) is all observations, \\(\\sum^M_{k=1}\\) is all components (number of clusters), \\(w_k\\) is component weights, and \\(N(x_i,\\mu_k, \\sigma^2_k)\\) are Gussians: \\[ \\begin{aligned} L(\\theta \\mid X_1,\\dots, X_n) &amp; = \\prod ^N_{i=1} \\sum^M_{k=1}w_k\\;N(x_i,\\mu_k, \\sigma^2_k) \\\\ ln L(\\theta) = \\log L(\\theta \\mid X_1,\\dots, X_n ) &amp;= \\sum^N_{i=1} \\log (\\sum^M_{k=1} w_k \\;N(x_i,\\mu_k, \\sigma^2_k)) \\\\ \\frac {dL}{d\\theta}ln L(\\theta) &amp;\\stackrel{Set}{= }0 \\end{aligned} \\] Unfortunately, the typical MLE approach reaches its limits since the above expression of Score function is analytically unsolvable. Thus, EM algorithm provides ways to overcome this unsolvable expression by maximizing the likelihood for the parameter of the interest and iterating till we have the desired estimate within the error range. 5.3 Train simple GMM example using EM Goal: Find which Gaussian component each observed data belongs to (latent variable \\(Z\\)), which can help we to identify each Gaussian parameters. 5.3.1 Set up Suppose we want to have a data set of 2 clusters in 1 dimension to simplified the example so \\(K=2\\) in \\(1D\\). Assumptions: We assume that we have observed variable \\(X\\) and some latent variable \\(Z\\) which decides which cluster the data points belong to. We also assume that the data given the cluster \\(J\\) has a Normal distributes with mean \\(\\mu_j\\) and variance \\(\\sigma^2_j\\) corresponding to that cluster. \\[ X \\mid Z = J \\sim N(\\mu_j, \\sigma^2_j) \\] Unknown parameters: In this case, we don’t know means \\(\\mu\\) and variances \\(\\sigma\\) for each clusters, and the probability of being in one of the 2 clusters \\(p\\). Since we only have 2 clusters, the probability of being in the other cluster is the complement probability of \\(p\\), or \\((1-p)\\). \\[ \\theta = (p,\\mu_0,\\mu_1,\\sigma_0,\\sigma_1) \\] 5.3.2 EM steps (Two-key-step version) Start with random guess for \\(\\theta\\) E step (finding the Q-function): \\(Q(\\theta \\mid \\theta^t) = E_{Z|X,\\theta^m} [\\log{p(x, z \\mid \\theta)}]\\) For a single observation: \\[ \\begin{aligned} p(X, Z\\mid \\theta) &amp; = p(x\\mid z, \\theta)\\;p(z\\mid\\theta) \\\\ &amp;= \\left( \\frac1{\\sqrt{2 \\pi}\\sigma_0}e^{-\\frac{(x-\\mu_0)^2}{2\\sigma_0^2}}(1-p)\\right)^{1-Z} \\;\\left( \\frac1{\\sqrt{2 \\pi}\\sigma_1}e^{-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}}p\\right)^{Z} \\end{aligned} \\] The above expression means that \\(p(x, z\\mid \\theta)\\) is equal to probability of the first cluster \\(z=0\\) times the probability of that \\((1-p)\\) and probability of the second cluster \\(z=1\\) times the probability of that \\(p\\). For a single observation: Q function \\[ \\begin{align} Q(\\theta \\mid \\theta^t) &amp; = E_{Z\\mid X,\\theta^t} [\\log{p(x,z\\mid \\theta)}] \\\\ &amp; = (1-E(z)) \\left(-\\log{\\sigma_0} - \\frac{(x-\\mu_0)^2}{x\\sigma_0^2}+\\log{(1-p)}+ const. \\right) \\\\ &amp; \\quad \\;+ E(z) \\left(-\\log{\\sigma_1} - \\frac{(x-\\mu_1)^2}{x\\sigma_1^2}+\\log{p}+ const. \\right) \\end{align} \\] Find the Q-function by calculating the expected \\(\\log {p(x, z\\mid \\theta)}\\) for a single observation. For n (iid) observation: Q function \\[ \\begin{aligned} Q(\\theta \\mid \\theta^t) &amp; = \\sum _i(1-E(z_i)) \\left(-\\log{\\sigma_0} - \\frac{(x-\\mu_0)^2}{x\\sigma_0^2}+\\log{(1-p)}+ const. \\right) \\\\ &amp; \\quad \\; + E(z_i) \\left(-\\log{\\sigma_1} - \\frac{(x-\\mu_1)^2}{x\\sigma_1^2}+\\log{p}+ const. \\right) \\end{aligned} \\] Generalize the Q-function from the above single observation by taking the sum of \\(z_i\\). For n (iid) observation: \\(E(z_i)\\) \\[ \\begin{aligned} E(z_i) &amp; = 0 \\;\\cdot \\; P(z_i = 0 \\mid x_i) + 1 \\;\\cdot \\; P(z_i = 1 \\mid x_i) = P(z_i = 1 \\mid x_i) \\\\ &amp; = \\frac{P(x_i\\mid z_i=1) P(z_i=1)}{P(x_i\\mid z_i=1) P(z_i=1) + P(x_i\\mid z_i=0) P(z_i=0)} \\\\ &amp; = \\frac{P(x_i\\mid z_i=1)\\;\\cdot\\; p}{P(x_i\\mid z_i=1)\\;\\cdot\\; p+ P(x_i\\mid z_i=0)\\;\\cdot\\; (1-p)} \\end{aligned} \\] Find the expected \\(z_i\\) in the above Q-function giving the known information of each observed \\(x_i\\). M step (finding the new guess for \\(\\theta\\) by maximizing the Q-function): \\(\\theta^{t+1} = \\underset{\\theta}{\\operatorname{argmax}} \\: Q(\\theta \\mid \\theta^t)\\) For the calculation below, we use weighed average, where the weights are the probability of being in the 1st “class”. \\[ \\begin{aligned} \\frac {dQ}{d\\mu_1} = \\sum_i E(z_i)\\left( \\frac{x_i-\\mu_1}{\\sigma_1^2} \\right) &amp; \\stackrel{Set}{= } 0 \\\\ \\therefore \\mu_1 &amp; = \\frac{\\sum E(z_i)x_i}{\\sum E(z_i)} \\\\ \\text{Similarly } \\mu_0 &amp; = \\frac{\\sum E(z_i)x_i}{\\sum E(z_i)}\\\\ \\\\ \\frac {dQ}{d\\sigma_1} = \\sum_i E(z_i)\\left( -\\frac1{\\sigma_1} +\\frac{(x_i-\\mu_1)^2}{\\sigma_1^3} \\right) &amp; \\stackrel{Set}{= } 0 \\\\ \\therefore \\sigma_1^2 &amp; = \\frac{\\sum E(z_i)(x_i-\\mu_1)^2}{\\sum E(z_i)} \\\\ \\text{Similarly } \\sigma_0^2 &amp; = \\frac{\\sum E(z_i)(x_i-\\mu_0)^2}{\\sum E(z_i)}\\\\ \\\\ \\frac {dQ}{dp} = \\sum_i \\left(-\\frac{1-E(z_i)}{1-p} + \\frac{E(z_i)}{p} \\right) &amp; \\stackrel{Set}{= } 0 \\\\ \\therefore p &amp; = \\frac{\\sum E(z_i)}{n} \\\\ \\end{aligned} \\] Repeat until \\(|\\theta^{t+1} - \\theta^m| &lt; \\epsilon\\) 5.4 R code example # EM: GMM library(ks) library(mvtnorm) library(extraDistr) library(ggplot2) library(pracma) 5.4.1 Simple example with 2 components in 1 dimension ## 1 dimension, 2 components ## sigma&#39;s, p are known; only estimate mu&#39;s set.seed(247) p = 0.3 n = 2000 # latent variable z z = rbinom(n, 1, p) # observed variables mu1 = 2 sig1= sqrt(2) mu0 = -2 sig0 = sqrt(1) x = rnorm(n, mu1, sig1)*z + rnorm(n, mu0, sig0)*(1-z) # plot data histogram hist(x, breaks = 50, col = &quot;#F3B941&quot;) # E step doE &lt;- function(theta){ mu0 = theta[1] mu1 = theta[2] E = rep(NA, n) for (i in 1:n){ a = dnorm(x[i], mean = mu1, sd = sig1)*p b = dnorm(x[i], mean = mu0, sd = sig0)*(1-p) E[i] = a/(a+b) } return(E) } # M step doM &lt;- function(E){ theta = rep(NA,2) theta[1] = sum((1-E)*x) / sum(1-E) theta[2] = sum(E*x) / sum(E) return(theta) } EM &lt;- function(theta, maxIter = 50, tol = 1e-5){ theta.t = theta for (i in 1:maxIter){ E = doE(theta.t) theta.t = doM(E) cat(&quot;Interation: &quot;, i, &quot; pt: &quot;, theta.t, &quot;\\n&quot;) if (norm(theta.t - theta, type = &quot;2&quot;) &lt; tol) break theta = theta.t } return(theta.t) } # Initial values theta0 = rmvnorm(1, mean = c(0,0)) (theta.final = EM(theta0)) ## Interation: 1 pt: 1.300618 -2.042688 ## Interation: 2 pt: 1.542635 -2.101853 ## Interation: 3 pt: 1.701525 -2.066578 ## Interation: 4 pt: 1.809297 -2.038246 ## Interation: 5 pt: 1.878507 -2.019401 ## Interation: 6 pt: 1.921065 -2.007542 ## Interation: 7 pt: 1.946516 -2.000335 ## Interation: 8 pt: 1.961482 -1.996054 ## Interation: 9 pt: 1.970195 -1.993545 ## Interation: 10 pt: 1.975238 -1.992088 ## Interation: 11 pt: 1.978148 -1.991245 ## Interation: 12 pt: 1.979822 -1.990759 ## Interation: 13 pt: 1.980786 -1.99048 ## Interation: 14 pt: 1.981339 -1.990319 ## Interation: 15 pt: 1.981657 -1.990227 ## Interation: 16 pt: 1.98184 -1.990174 ## Interation: 17 pt: 1.981945 -1.990143 ## Interation: 18 pt: 1.982005 -1.990126 ## Interation: 19 pt: 1.98204 -1.990115 ## Interation: 20 pt: 1.98206 -1.99011 ## Interation: 21 pt: 1.982071 -1.990106 ## Interation: 22 pt: 1.982078 -1.990104 ## [1] 1.982078 -1.990104 ## estimate p, mu&#39;s and sigma&#39;s 5.4.2 General case We also include the code for a more general version of EM algorithm in GMM questions for simulation and hand-on further exploration. If you are curious about the calculation for the following general case, look at the second half of the youtube video. ## K * 2D Gaussians set.seed(247) # Params K = 3 phis = rdirichlet(1, rep(1, K)) j = 2 # how far apart the centers are mus = matrix(c(j,-j,0,j,j,-j), ncol = 2) # rmvnorm(K, mean = c(0, 0)) Sigmas = vector(&quot;list&quot;, K) for (k in 1:K){ mat = matrix(rnorm(100), ncol = 2) Sigmas[[k]] = cov(mat) } # Latent z = rcat(n, phis) # Observed (generated) x = matrix(nrow = n, ncol = 2) for (i in 1:n){ k = z[i] mu = mus[k,] sigma = Sigmas[[k]] x[i,] = rmvnorm(1, mean = mu, sigma = sigma) } # plot the data df = data.frame(x = x, mu = as.factor(z)) ggplot(df, aes(x = x[,1], y = x[,2], col = mu, fill = mu)) + geom_point() + scale_color_manual(values = c(&quot;#3B5BA5&quot;, &quot;#E87A5D&quot;, &quot;#F3B941&quot;)) + theme_minimal() # E step doE &lt;- function(theta){ E = with(theta, do.call(cbind, lapply(1:K, function(k) phis[[k]]*dmvnorm(x, mus[[k]], Sigmas[[k]])))) E/rowSums(E) } # M step doM &lt;- function(E){ phis = colMeans(E) covs = lapply(1:K, function(k) cov.wt(x, E[, k], method = &quot;ML&quot;)) mus = lapply(covs, &quot;[[&quot;, &quot;center&quot;) sig = lapply(covs, &quot;[[&quot;, &quot;cov&quot;) return(list(mus = mus, Sigmas = sig, phis = phis)) } logLikelihood &lt;- function(theta){ probs = with(theta, do.call(cbind, lapply(1:K, function(i) phis[i] * dmvnorm(x, mus[[i]], Sigmas[[i]])))) sum(log(rowSums(probs))) } EM &lt;- function(theta, maxIter = 30, tol = 1e-1){ theta.t = theta for (i in 1:maxIter){ E = doE(theta.t) theta.t = doM(E) ll.diff = logLikelihood(theta.t) - logLikelihood(theta) cat(&quot;Iteration: &quot;, i, &quot; ll differnece: &quot;, ll.diff, &quot;\\n&quot;) if (abs(ll.diff) &lt; tol) break theta = theta.t } return(theta.t) } # inital values set.seed(1) phis0 = rdirichlet(1, rep(1,K)) mus0 = vector(&quot;list&quot;, K) Sigmas0 = vector(&quot;list&quot;, K) for (k in 1:K){ mat = matrix(rnorm(100), ncol = 2) mus0[[k]] = rmvnorm(1, mean = c(3,-3)) Sigmas0[[k]] = cov(mat) } theta0 = list(mus = mus0, Sigmas = Sigmas0, phis = phis0) (theta.final = EM(theta0)) ## Iteration: 1 ll differnece: 27206.85 ## Iteration: 2 ll differnece: 195.7352 ## Iteration: 3 ll differnece: 124.3022 ## Iteration: 4 ll differnece: 43.09508 ## Iteration: 5 ll differnece: 14.00595 ## Iteration: 6 ll differnece: 9.720739 ## Iteration: 7 ll differnece: 10.52907 ## Iteration: 8 ll differnece: 13.34927 ## Iteration: 9 ll differnece: 18.1401 ## Iteration: 10 ll differnece: 23.50497 ## Iteration: 11 ll differnece: 27.55922 ## Iteration: 12 ll differnece: 28.71809 ## Iteration: 13 ll differnece: 25.69147 ## Iteration: 14 ll differnece: 20.10685 ## Iteration: 15 ll differnece: 15.67748 ## Iteration: 16 ll differnece: 14.31082 ## Iteration: 17 ll differnece: 15.47378 ## Iteration: 18 ll differnece: 18.33269 ## Iteration: 19 ll differnece: 22.13416 ## Iteration: 20 ll differnece: 25.82935 ## Iteration: 21 ll differnece: 27.74146 ## Iteration: 22 ll differnece: 25.48004 ## Iteration: 23 ll differnece: 18.18744 ## Iteration: 24 ll differnece: 9.540094 ## Iteration: 25 ll differnece: 3.847487 ## Iteration: 26 ll differnece: 1.322018 ## Iteration: 27 ll differnece: 0.4200287 ## Iteration: 28 ll differnece: 0.1289125 ## Iteration: 29 ll differnece: 0.03897584 ## $mus ## $mus[[1]] ## [1] -0.03534303 -1.99996843 ## ## $mus[[2]] ## [1] 2.011046 2.027221 ## ## $mus[[3]] ## [1] -2.007137 2.070099 ## ## ## $Sigmas ## $Sigmas[[1]] ## [,1] [,2] ## [1,] 0.9973681 -0.0912405 ## [2,] -0.0912405 1.0528290 ## ## $Sigmas[[2]] ## [,1] [,2] ## [1,] 0.75036568 0.04210181 ## [2,] 0.04210181 0.77091873 ## ## $Sigmas[[3]] ## [,1] [,2] ## [1,] 0.71945002 -0.02774193 ## [2,] -0.02774193 0.89936925 ## ## ## $phis ## [1] 0.1734057 0.3432008 0.4833934 5.5 References: Expectation-Maximization for GMMs explained Youtube video: EM - GMM Example "],["application-2-hidden-markov-model.html", "Topic 6 Application 2: Hidden Markov Model 6.1 Overview of Hidden Markov Model 6.2 Baum-Welch Algorithm 6.3 Implementation in R 6.4 References:", " Topic 6 Application 2: Hidden Markov Model 6.1 Overview of Hidden Markov Model 6.1.1 Markov Chain Markov Chain Intuition Before formally defining what a Markov chain is, consider this: Imagine you’re a weather forecaster and you want to predict whether it will be sunny, cloudy, or rainy tomorrow based on the weather conditions of the past few days. Let’s assume that the probability of tomorrow’s weather only depends on today’s weather and not on the weather before that. Suppose that, if it’s sunny today, there is a 60% chance of it being sunny tomorrow, a 30% chance of it being cloudy, and a 10% chance of it being rainy. \\[ \\begin{align*} &amp;\\mathbb{P}(\\text{sunny tmr}\\mid \\text{sunny today}) = 0.6 \\\\ &amp;\\mathbb{P}(\\text{cloudy tmr}\\mid \\text{sunny today}) = 0.3 \\\\ &amp;\\mathbb{P}(\\text{rainy tmr}\\mid \\text{sunny today}) = 0.1 \\\\ \\end{align*} \\] If it’s cloudy today, there is a 20% chance of it being sunny tomorrow, a 70% chance of it still being cloudy, and a 10% chance of it being rainy. \\[ \\begin{align*} &amp;\\mathbb{P}(\\text{sunny tmr}\\mid \\text{cloudy today}) = 0.2 \\\\ &amp;\\mathbb{P}(\\text{cloudy tmr}\\mid \\text{cloudy today}) = 0.7 \\\\ &amp;\\mathbb{P}(\\text{rainy tmr}\\mid \\text{cloudy today}) = 0.1 \\\\ \\end{align*} \\] If it’s rainy today, there is a 30% chance of it being sunny tomorrow, a 40% chance of it still being cloudy, and a 30% chance of it being rainy. \\[ \\begin{align*} &amp;\\mathbb{P}(\\text{sunny tmr}\\mid \\text{rainy today}) = 0.3 \\\\ &amp;\\mathbb{P}(\\text{cloudy tmr}\\mid \\text{rainy today}) = 0.4 \\\\ &amp;\\mathbb{P}(\\text{rainy tmr}\\mid \\text{rainy today}) = 0.3 \\\\ \\end{align*} \\] In informal “mathematical” notation, we can represent this as: \\[ \\text{Weather States} = \\{\\text{Sunny}, \\text{Cloudy}, \\text{Rainy}\\} \\] \\[ \\text{Transition Probabilities Matrix} = \\begin{bmatrix} 0.6 &amp; 0.3 &amp; 0.1 \\\\ 0.2 &amp; 0.7 &amp; 0.1 \\\\ 0.3 &amp; 0.4 &amp; 0.3 \\end{bmatrix} \\] In this case (and with our assumptions above), the sequence of the weather states overtime, say {Sunny, Sunny, Sunny, Cloudy, Cloudy, Rainy, Cloudy,…}, is a classic Markov chain. This is because the weather tomorrow only depends on the weather today, or more broadly, the state at time \\(t+1\\) only depends on the state at time \\(t\\). Markov Chain Definition Mathematically, we can denote a Markov chain by \\[ X=(X_n)_{n\\in\\mathbb{N}} = (X_0, X_1, X_2,...) \\] where at each timestamp the process takes its values in a discrete set \\(E\\) such that \\[ X_n \\in E \\quad \\forall n\\in \\mathbb{N} \\] Then, the Markov property implies that we have \\[ \\mathbb{P}(X_{n+1}=s_{n+1}\\mid X_n = s_n, X_{n-1}=s_{n-1},X_{n-2}=s_{n-2},\\cdots) = \\mathbb{P}(X_{n+1}=s_{n+1}\\mid X_n = s_n) \\] 6.1.2 Hidden Markov Model Hidden Markov Model Intuition To illustrate Hidden Markov Model, we can continue with the weather example. Let’s assume that we are observing whether the grass is wet or dry every day, but we don’t directly observe the weather. That being said, we only observe the effect of the weather on the grass, and we assume whether the grass is wet or dry only depends on the weather that day. \\[ \\text{Weather States} = \\{\\text{Sunny}, \\text{Cloudy}, \\text{Rainy}\\} \\] \\[ \\text{Grass States} = \\{\\text{Dry}, \\text{Wet}\\} \\] Suppose that, if it’s sunny today, there is a \\(100 \\%\\) chance of observing dry grass, a \\(0 \\%\\) chance of observing wet grass. \\[ \\mathbb{P}(\\text{dry grass}\\mid \\text{Sunny}) = 1 \\\\ \\mathbb{P}(\\text{wet grass}\\mid \\text{Sunny}) = 0 \\] Suppose that, if it’s sunny today, there is a \\(70 \\%\\) chance of observing dry grass, a \\(30 \\%\\) chance of observing wet grass. \\[ \\mathbb{P}(\\text{dry grass}\\mid \\text{Cloudy}) = 0.7 \\\\ \\mathbb{P}(\\text{wet grass}\\mid \\text{Cloudy}) = 0.3 \\] Suppose that, if it’s rainy today, there is a \\(10 \\%\\) chance of observing dry grass, a \\(90 \\%\\) chance of observing wet grass. \\[ \\mathbb{P}(\\text{dry grass}\\mid \\text{Cloudy}) = 0.1 \\\\ \\mathbb{P}(\\text{wet grass}\\mid \\text{Cloudy}) = 0.9 \\] Aggregating the probabilities above into one matrix to get the emission probabilities matrix. \\[ \\text{Emission Probabilities Matrix} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0.7 &amp; 0.3 \\\\ 0.1 &amp; 0.9 \\end{bmatrix} \\] Hidden Markov Model Definition A Hidden Markov Model (HMM) is a statistical model that represents a sequence of observable events, \\[ Y = (Y_t)_{1\\leq t\\leq T} = (Y_1, Y_2, ..., Y_T) \\] ,where each observation \\(Y_t\\) is generated by an unknown or hidden state sequence that is a Markov chain \\[ X = (X_t)_{1\\leq t\\leq T} = (X_1, X_2, ..., X_T) \\] , which is not directly observed. Formally, an HMM is defined by the following parameters: Hidden states in sequence X take values in a finite set \\((h_1,h_2,...,h_N)\\). Observations in sequence Y take values in a finite set \\((o_1,o_2,...,o_M)\\). An initial probability distribution over the hidden states \\(X = (X_t)_{1\\leq t\\leq T}\\), which is given by \\(\\pi = (\\pi_1, \\pi_2, ..., \\pi_N)\\) such that \\(\\pi_i = \\mathbb{P}(X_1 = h_i)\\). A transition probability matrix \\(A_{N\\times N}\\) = {\\(a_{ij}\\)}, where entry \\(a_{ij}\\) represents the probability of transitioning from state \\(h_i\\) to state \\(h_j\\). \\[ a_{ij} = \\mathbb{P}(X_{t+1} = h_j \\mid X_{t} = h_i) \\] An emission probability matrix \\(B_{N\\times M}\\) = {\\(b_{j(k)}\\)}, where \\(b_{jk} = b_{j}(o_k)\\) represents the probability of observing the \\(o_k\\) given that the system is in state \\(h_j\\). \\[ b_{jk} = b_{j}(o_{k}) = \\mathbb{P}(Y_t = o_k \\mid X_t = h_j) \\] The goal of an HMM is to estimate the hidden state sequence X given the observed sequence Y, or to compute the likelihood of the observed sequence Y given the model parameters \\((\\pi, A, B)\\). In fact, we can describe a hidden markov model by \\(\\theta = (\\pi, A, B)\\). In the next section, we will apply EM algorithm to estimate parameters of HMM model, which has a specific name – Baum-Welch Algorithm. 6.2 Baum-Welch Algorithm 6.2.1 Intuition The Baum-Welch algorithm finds a local maximum for \\(\\theta^{*} = \\text{argmax}_{\\theta}\\mathbb{P}(Y\\mid \\theta)\\). To put it in words, it finds the set of parameters \\(\\theta^{*}\\) that makes the observed sequence \\(Y\\) mostly likely to have occurred. 6.2.2 Algorithm in steps Step 1: Initial Guess Set \\(\\theta^{t=0} = (A^{t=0}, B^{t=0}, \\pi^{t=0})\\), where the guesses could be random or not depending on if no prior knowledge is available. Step 2: Forward-Backward Algorithm Let’s define some new variables for each observation in the sequence here – the forward probabilities and the backward probabilities. Definition of forward and backward probabilities For each observation \\(Y_{t} \\in Y\\), the forward probabilities, denoted as \\(\\alpha_t(i)\\), which represent the probability of \\(Y_{t}\\) being in state \\(o_i\\) at time \\(t\\) given the observations up to time \\(t\\), \\((Y_1, Y_2,...,Y_{t-1})\\). \\[ \\alpha_t(i) = \\mathbb{P}(Y_t = o_i\\mid Y_1 = y_1, Y_2 = y_2,...,Y_{t-1} = y_{t-1}) \\] , where \\(y = (y_t)_{0&lt;t\\leq T}\\) is the true observation sequence. On the other hand, the backward probabilities, denoted as \\(\\beta_t(i)\\), represent the probability of starting from state \\(o_i\\) at time \\(t\\) and generating the remaining observations from time \\(t+1\\) to the end of the sequence, \\((Y_{t+1},...,Y_T)\\). \\[ \\beta_t(i) = \\mathbb{P}(Y_t = o_i\\mid Y_{t+1} = y_{t+1}, Y_{t+2} = y_{t+2},...,Y_{T} = y_{T}) \\] Computation of forward and backward probabilities The forward probabilities can be computed using the following recursive formulas: \\[ \\begin{aligned} \\alpha_1(i) &amp;= \\pi_i \\cdot b_i(y_1), \\quad 1\\leq i \\leq N\\\\ \\alpha_t(i) &amp;= b_i(y_t) \\sum_{j=1}^{N} \\alpha_{t-1}(j) \\cdot a_{ji}, \\quad 1\\leq i \\leq N, \\quad 2 \\leq t \\leq T \\\\ \\end{aligned} \\] , where \\(N\\) is the number of states in the HMM, \\(a_{ij}\\) is the transition probability from state \\(h_i\\) to state \\(h_j\\), \\(b_i(y_t)\\) is the emission probability of observation \\(y_t\\) from state \\(h_i\\), and \\(y_t\\) is the true observation at time \\(t\\). The backward probabilities can be computed using similar recursive formulas: \\[ \\begin{aligned} \\beta_T(i) &amp;= 1, \\quad 1\\leq i \\leq N\\\\ \\beta_t(i) &amp;= \\sum_{j=1}^{N} a_{ij} \\cdot b_j(y_{t+1}) \\cdot \\beta_{t+1}(j), \\quad 1\\leq i \\leq N,\\quad 1 \\leq t \\leq T-1 \\end{aligned} \\] , where \\(N\\) is the number of states in the HMM, \\(a_{ij}\\) is the transition probability from state \\(h_i\\) to state \\(h_j\\), \\(b_i(y_t)\\) is the emission probability of observation \\(y_t\\) from state \\(h_i\\), and \\(y_t\\) is the true observation at time \\(t\\). Step 3: E-step Calculate two new sets of quantities: \\(\\gamma_t(i)\\) represents the probability of the hidden state at time \\(t\\), \\(X_t\\), being in state \\(h_i\\) given the entire observation sequence \\(Y=y\\) and parameter set \\(\\theta\\), and it can be calculated using Bayes’ theorem. \\[ \\begin{aligned} \\gamma_t(i) &amp;= \\mathbb{P}(X_t = h_i\\mid Y = y,\\theta) \\\\ &amp;= \\frac{\\mathbb{P}(X_t = h_i, Y = y \\mid \\theta)}{\\mathbb{P}(Y=y \\mid \\theta)} \\\\ &amp;= \\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{j=1}^{N}\\alpha_t(j)\\beta_t(j)} \\end{aligned} \\] \\(\\xi_t(i,j)\\) represents probability of the hidden state being in state \\(h_i\\) at time \\(t\\) and and being in state \\(h_j\\) at time \\(t+1\\) given the the entire observation sequence \\(Y=y\\) and parameter set \\(\\theta\\), and it can also be calculated using the Bayes’ Rule. \\[ \\begin{aligned} \\xi_t(i,j) &amp;= \\mathbb{P}(X_t=h_i, X_{t+1}=h_j\\mid Y=y,\\theta) \\\\ &amp;= \\frac{\\mathbb{P}(X_t = h_i, X_{t+1}=h_j,Y=y\\mid \\theta)}{\\mathbb{P}(Y=y\\mid \\theta)} \\\\ &amp;= \\frac{\\alpha_t(i)a_{ij}\\beta_{t+1}(j)b_j(y_{t+1})}{\\sum_{k=1}^{N}\\sum_{w=1}^{N}\\alpha_t(k)a_{kw}\\beta_{t+1}(w)b_w(y_{t+1})} \\end{aligned} \\] Notice here, that the denominators of \\(\\gamma_t(i)\\) and \\(\\xi_t(i,j)\\) are the same, and it calculates the probability of observing the sequence \\(y\\) that we observed under our current guess for \\(\\theta\\). Step 4: M-step Now, we can update the parameters of the hidden markov model. First, let’s update the initial probability distribution \\(\\pi = (\\pi_1, \\pi_2, ..., \\pi_N)\\). \\[ \\begin{aligned} \\hat{\\pi}_i &amp;= \\gamma_1(i) \\\\ &amp;= \\mathbb{P}(X_1 = h_i) \\\\ \\\\ \\hat{\\pi} &amp;= (\\gamma_1(1), \\gamma_1(2),...,\\gamma_1(N)) \\\\ &amp;= (\\mathbb{P}(X_1 = h_1), \\mathbb{P}(X_1 = h_2),...,\\mathbb{P}(X_1 = h_N)) \\\\ \\end{aligned} \\] Next, update the \\(a_{ij}\\) entry in the transition matrix \\(A\\) with the expected number of transitions from state \\(h_i\\) to state \\(h_j\\) divided by the expected total number of transitions from state \\(h_i\\) including transitions to itself which equals to the number of times state \\(h_i\\) appears in the observed sequence \\(y\\). \\[ \\hat{a}_{ij} = \\frac{\\sum_{t=1}^{T-1}\\xi_{t}(i,j)}{\\sum_{t=1}^{T-1}\\gamma_{t}(i)} \\] Last but not least, update \\(b_{ij}\\) entry in the emission matrix \\(B\\) with the expected number of times the output observations have been equal to \\(o_j\\) under hidden state \\(h_i\\) over the expected total number of \\(h_i\\) in the hidden states sequence \\(X\\). \\[ \\hat{b}_i(o_k) = \\frac{\\sum_{t=1}^{T}1_{y_t = o_k}\\gamma_i(t)}{\\sum_{t=1}^{T}\\gamma_i(t)} \\] , where \\[ 1_{y_t = o_k} = \\Bigg\\{^{ 1 \\text{ if y_t = o_k}}_{0 \\text{ otherwise}} \\] Step 5: Determine convergence status and repeat iteratively if needed Set a criteria for whether the estimation sequence is converging and our current guess is close enough to last guess. If not, then repeat from step 2 to step 5 iteratively until convergence. Alternatively, we can set a maximum number of iteration for the Baum-Welch algorithm, n. Once we have completed n iterations, the algorithm stops and we take the estimates from the last iteration as results. 6.3 Implementation in R 6.3.1 HMM package HMM package documentation: https://cran.r-project.org/web/packages/HMM/HMM.pdf #install.packages(&#39;HMM&#39;) library(HMM) Example usage of ‘baumWelch’ function in the HMM package: # Initialize HMM with states values sets, transition probabilities, and emission probabilities hmm = initHMM(c(&quot;A&quot;,&quot;B&quot;) # hidden states A and B ,c(&quot;L&quot;,&quot;R&quot;) # observation states L or R ,startProbs = c(0.5,0.5) ,transProbs=matrix(c(.9,.1,.1,.9),2) ,emissionProbs=matrix(c(.5,.51,.5,.49),2)) # print the HMM we just built to check print(hmm) ## $States ## [1] &quot;A&quot; &quot;B&quot; ## ## $Symbols ## [1] &quot;L&quot; &quot;R&quot; ## ## $startProbs ## A B ## 0.5 0.5 ## ## $transProbs ## to ## from A B ## A 0.9 0.1 ## B 0.1 0.9 ## ## $emissionProbs ## symbols ## states L R ## A 0.50 0.50 ## B 0.51 0.49 # Sequence of observation a = sample(c(rep(&quot;L&quot;,100),rep(&quot;R&quot;,300))) b = sample(c(rep(&quot;L&quot;,300),rep(&quot;R&quot;,100))) observation = c(a,b) # append vector b after vector a # Baum-Welch bw = baumWelch(hmm,observation, maxIteration = 10) # Input: hmm, observation, maxIteration (the maximum number of iterations in the Baum-Welch algorithm) print(bw$hmm) ## $States ## [1] &quot;A&quot; &quot;B&quot; ## ## $Symbols ## [1] &quot;L&quot; &quot;R&quot; ## ## $startProbs ## A B ## 0.5 0.5 ## ## $transProbs ## to ## from A B ## A 9.974699e-01 0.00253015 ## B 6.670293e-06 0.99999333 ## ## $emissionProbs ## symbols ## states L R ## A 0.2464713 0.7535287 ## B 0.7486563 0.2513437 Notice here that one drawback of the baumWelch function in the HMM package is that it treats the initial probability vector we put in as true initial probability rather than our random guess. Thus, it does not try to update initial probability, which lower the accuracy of the estimation when the initial probability vector we put in is not that accurate. 6.3.2 Revisit Weather-Grass Example Let’s do a simulation to test the performance of Baum-Welch algorithm here. # Initialize HMM with states values sets, transition probabilities, and emission probabilities hmm = initHMM(c(&quot;Sunny&quot;,&quot;Cloudy&quot;,&quot;Rainy&quot;), # hidden states: WEATHER * 3 c(&quot;Dry&quot;,&quot;Wet&quot;), # observation states: GRASS STATUS * 2 startProbs = c(1/3,1/3,1/3), transProbs = cbind(c(.6,.2,.3),c(.3,.4,.7),c(.1,.1,.3)), emissionProbs=rbind(c(1,0),c(.7,.3),c(.1,.9))) # print the HMM we just built to check print(hmm) ## $States ## [1] &quot;Sunny&quot; &quot;Cloudy&quot; &quot;Rainy&quot; ## ## $Symbols ## [1] &quot;Dry&quot; &quot;Wet&quot; ## ## $startProbs ## Sunny Cloudy Rainy ## 0.3333333 0.3333333 0.3333333 ## ## $transProbs ## to ## from Sunny Cloudy Rainy ## Sunny 0.6 0.3 0.1 ## Cloudy 0.2 0.4 0.1 ## Rainy 0.3 0.7 0.3 ## ## $emissionProbs ## symbols ## states Dry Wet ## Sunny 1.0 0.0 ## Cloudy 0.7 0.3 ## Rainy 0.1 0.9 # Simulate 5000 observations and corresponding hidden states consecutively sim = simHMM(hmm, 5000) # Baum-Welch bw = baumWelch(hmm,sim$observation, maxIteration = 10) print(bw$hmm) ## $States ## [1] &quot;Sunny&quot; &quot;Cloudy&quot; &quot;Rainy&quot; ## ## $Symbols ## [1] &quot;Dry&quot; &quot;Wet&quot; ## ## $startProbs ## Sunny Cloudy Rainy ## 0.3333333 0.3333333 0.3333333 ## ## $transProbs ## to ## from Sunny Cloudy Rainy ## Sunny 0.6481263 0.2120997 0.1397740 ## Cloudy 0.3535154 0.4445554 0.2019293 ## Rainy 0.2973958 0.4071881 0.2954161 ## ## $emissionProbs ## symbols ## states Dry Wet ## Sunny 1.0000000 0.0000000 ## Cloudy 0.7087220 0.2912780 ## Rainy 0.1058077 0.8941923 6.4 References: HMM package documentation HMM wikipedia "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
