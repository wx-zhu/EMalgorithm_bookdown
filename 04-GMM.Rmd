# Application 2: Gaussian Mixture Model

## Why using GMMs?

Suppose we have a dataset with 2 features. When you plot your dataset, it might look like this:

```{r, echo=FALSE, fig.align='center', out.width='100%', fig.cap='A dataset with 2 clusters, made by Maël Fabien'}
knitr::include_graphics("image/pic1.png")
```

From the above plot, there are 2 clusters. Thus, we are aiming to solve two tasks: 

1. Clustering of the data

2. Modeling of the distribution for the data and the corresponding parameters. 

Usually, when doing clustering, people will think of [k-means](https://en.wikipedia.org/wiki/K-means_clustering). But, in fact, K-means is just a special case for Gaussian Mixture Models (GMMs) when using a specific EM algorithm. The following plot shows some differences between K-means and GMMs for clustering. In general, for k-means, the clusters are defined by the data means whereas GMM, clusters are defined by data means and variance modeled as Gaussian (aka Normal distribution).

```{r, echo=FALSE, fig.align='center', out.width='100%', fig.cap='K-Means vs. GMMs for clustering, made by Maël Fabien'}
knitr::include_graphics("image/pic2.png")
```

Thus, when using 2 Gaussians, we want to find:

1. the mean and covariance of the first Gaussian

2. the mean and covariance of the second Gaussian

3. the weight of each Gaussian component


## Train GMM using MLE

When training a GMM, we want to identify a set of parameters that characterize our GMM. 

\[
\theta = (w_k, \mu_k, \Sigma_k), k = 1,2,3,\dots, M
\]

Similar to solve parameters in **single** Gaussian model using [MLE](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation), we can take the *Score function* $\frac {dL}{d\theta}ln L(\theta)$ using log-likelihood and set it equal to 0 again, where $\prod^N_{i=1}$ is all observations, $\sum^M_{k=1}$ is all components (number of clusters), $w_k$ is component weights, and $N(x_i,\mu_k, \sigma^2_k)$ are Gussians: 

\[
\begin{aligned}
L(\theta \mid X_1,\dots, X_n) & = \prod ^N_{i=1} \sum^M_{k=1}w_k\;N(x_i,\mu_k, \sigma^2_k) \\
ln L(\theta)  = \log L(\theta \mid X_1,\dots, X_n ) &= \sum^N_{i=1} \log (\sum^M_{k=1} w_k \;N(x_i,\mu_k, \sigma^2_k)) \\
\frac {dL}{d\theta}ln L(\theta) &\stackrel{Set}{= }0
\end{aligned}
\]

Unfortunately, the typical MLE approach reaches its limits since the above expression of **Score function** is analytically **unsolvable**. Thus, EM algorithm provides ways to overcome this unsolvable expression by maximizing the likelihood for the parameter of the interest and iterating till we have the desired estimate within the error range. 

## Train simple GMM example using EM 

* Goal: Find which Gaussian component each observed data belongs to (**latent** variable $Z$), which can help we to identify each Gaussian parameters.

### Set up 

Suppose we want to have a data set of *2 clusters in 1 dimension* to simplified the example so $K=2$ in $1D$. 

* Assumptions:
We assume that we have observed variable $X$ and some latent variable $Z$ which decides which cluster the data points belong to. We also assume that the data given the cluster $J$ has a *Normal* distributes with mean $\mu_j$ and variance $\sigma^2_j$ corresponding to that cluster. 

\[
X \mid Z = J \sim N(\mu_j, \sigma^2_j)
\]


* Unknown parameters: 
In this case, we don't know means $\mu$ and variances $\sigma$ for each clusters, and the probability of being in one of the 2 clusters $p$. Since we only have 2 clusters, the probability of being in the other cluster is the complement probability of $p$, or $(1-p)$.

\[
\theta = (p,\mu_0,\mu_1,\sigma_0,\sigma_1)
\]

### EM steps (Two-key-step version)

1. Start with random guess for $\theta$ 


2. E step (finding the Q-function): $Q(\theta \mid \theta^t) = E_{Z|X,\theta^m} [\log{p(x, z \mid \theta)}]$

    - **For a single observation: **
\[
\begin{aligned}
p(X, Z\mid \theta) & = p(x\mid z, \theta)\;p(z\mid\theta) \\
&= \left( \frac1{\sqrt{2 \pi}\sigma_0}e^{-\frac{(x-\mu_0)^2}{2\sigma_0^2}}(1-p)\right)^{1-Z} \;\left( \frac1{\sqrt{2 \pi}\sigma_1}e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}}p\right)^{Z} 
\end{aligned}
\]
The above expression means that $p(x, z\mid \theta)$ is equal to probability of the first cluster $z=0$ times the probability of that $(1-p)$ and probability of the second cluster $z=1$ times the probability of that $p$.

    - **For a single observation: Q function **
   \[
   \begin{align}
        Q(\theta \mid \theta^t) & = E_{Z\mid X,\theta^t} [\log{p(x,z\mid \theta)}] \\ 
        & = (1-E(z)) \left(-\log{\sigma_0} - \frac{(x-\mu_0)^2}{x\sigma_0^2}+\log{(1-p)}+ const. \right) \\ 
        & \quad \;+ E(z) \left(-\log{\sigma_1} - \frac{(x-\mu_1)^2}{x\sigma_1^2}+\log{p}+ const. \right)
    \end{align} 
   \]  
Find the Q-function by calculating the expected $\log {p(x, z\mid \theta)}$ for a single observation.

    - **For n (iid) observation: Q function **
   \[
   \begin{aligned}
        Q(\theta \mid \theta^t)  
        & = \sum _i(1-E(z_i)) \left(-\log{\sigma_0} - \frac{(x-\mu_0)^2}{x\sigma_0^2}+\log{(1-p)}+ const. \right) \\
        & \quad \; + E(z_i) \left(-\log{\sigma_1} - \frac{(x-\mu_1)^2}{x\sigma_1^2}+\log{p}+ const. \right)
    \end{aligned} 
   \]  
Generalize the Q-function from the above single observation by taking the sum of $z_i$. 

    - **For n (iid) observation:** $E(z_i)$
\[
\begin{aligned}
E(z_i) & = 0 \;\cdot \; P(z_i = 0 \mid x_i) + 1 \;\cdot \; P(z_i = 1 \mid x_i) = P(z_i = 1 \mid x_i) \\
& = \frac{P(x_i\mid z_i=1) P(z_i=1)}{P(x_i\mid z_i=1) P(z_i=1) + P(x_i\mid z_i=0) P(z_i=0)} \\
& = \frac{P(x_i\mid z_i=1)\;\cdot\; p}{P(x_i\mid z_i=1)\;\cdot\; p+ P(x_i\mid z_i=0)\;\cdot\; (1-p)}
\end{aligned}
\]
Find the expected $z_i$ in the above Q-function giving the known information of each observed $x_i$.


3. M step (finding the new guess for $\theta$ by maximizing the Q-function): $\theta^{t+1} = \underset{\theta}{\operatorname{argmax}} \: Q(\theta \mid \theta^t)$

For the calculation below, we use weighed average, where the weights are the probability of being in the 1st "class". 
\[
\begin{aligned}
\frac {dQ}{d\mu_1} = \sum_i E(z_i)\left( \frac{x_i-\mu_1}{\sigma_1^2} \right) & \stackrel{Set}{= } 0 \\
\therefore \mu_1 & = \frac{\sum E(z_i)x_i}{\sum E(z_i)} \\
\text{Similarly } \mu_0 & = \frac{\sum E(z_i)x_i}{\sum E(z_i)}\\
\\
\frac {dQ}{d\sigma_1} = \sum_i E(z_i)\left( -\frac1{\sigma_1} +\frac{(x_i-\mu_1)^2}{\sigma_1^3} \right) & \stackrel{Set}{= } 0 \\
\therefore \sigma_1^2 & = \frac{\sum E(z_i)(x_i-\mu_1)^2}{\sum E(z_i)} \\
\text{Similarly } \sigma_0^2 & = \frac{\sum E(z_i)(x_i-\mu_0)^2}{\sum E(z_i)}\\
\\
\frac {dQ}{dp} = \sum_i \left(-\frac{1-E(z_i)}{1-p} + \frac{E(z_i)}{p} \right) & \stackrel{Set}{= } 0 \\
\therefore p & = \frac{\sum E(z_i)}{n} \\
\end{aligned}
\]

4. Repeat until $|\theta^{t+1} - \theta^m| < \epsilon$


## R code example




## References: 

1. [Expectation-Maximization for GMMs explained](https://towardsdatascience.com/expectation-maximization-for-gmms-explained-5636161577ca)

